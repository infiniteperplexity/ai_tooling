{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, let's take a break from beating my head against the divergence wall and instead implement some things.  First off, warmup and scheduling.\n",
    "# Okay nice, with gradient clipping it seems to not be diverging at least after three step blocks.  So the one additional thing to try after this would be to disble warmup but keep gradient clipping.\n",
    "# Yep, it didn't diverge finally!  Okay, but if we disable warmup, it does still diverge.  Which is what the literature suggests will happen.  Note that I still didn't use pre-Norm ordering.\n",
    "# So what's next?  I could either incorporate some of these modifications I've been making in the forked module, or I could try to implement FixUp or similar things.\n",
    "# Okay, I think I successfully refactored the MixerModel class to make it much more flexible and modular, and importantly, to make Pre-LN the default.\n",
    "# So next would probably be warmup-replacing initialization schemes.\n",
    "# I guess the other big thing is quantization and mixed precision.  I'm scared of that.  But I think I would learn a ton.  Actually am I ready to-rebaseline?\n",
    "# Okay so one thing here: It's true that things like ReZero and FixUp are \"tweaks\", but they are tweaks that might be especially relevant to the testing scenario I'm using, where I want to use the early stages of training to get insight into the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # There are going to be some things we want to initialize lazily to economize on resources and reuse constructor calls.\n",
    "# everything will use the same tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral, padding_side = \"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267_477_061\n",
      "Max length: 236695, estimated tokens: 87_664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2f658cd405a43e2aebb2522d2a905fe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6749637324d940bcb12c99bd01b0f903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Everything will use the same dataset and dataloaders\n",
    "repo = \"wikimedia/wikipedia\"\n",
    "import datasets\n",
    "ds = datasets.load_dataset(repo, \"20231101.simple\")\n",
    "def quick_estimate_tokens(ds, field=\"text\", chars_per_token=2.7):\n",
    "    tally = 0\n",
    "    max_len = 0\n",
    "    lengths = {}\n",
    "    for row in ds:\n",
    "        l = len(row[field])\n",
    "        tally += l\n",
    "        lengths[l] = lengths.get(l, 0) + 1\n",
    "        if l > max_len:\n",
    "            max_len = l\n",
    "\n",
    "    print(f'{int(tally):_}')\n",
    "    print(f'Max length: {max_len}, estimated tokens: {int(max_len / chars_per_token):_}')\n",
    "    lengths = list(lengths.items())\n",
    "    lengths.sort(reverse=True)\n",
    "    return int(tally/chars_per_token), lengths\n",
    "\n",
    "total, length = quick_estimate_tokens(ds['train'], field=\"text\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "max_tokens = 512\n",
    "def batch_tokenize(batch):\n",
    "    return {\"input_ids\": tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_tokens).input_ids}\n",
    "\n",
    "tokenized = ds.map(batch_tokenize, batched=True, batch_size=1000)\n",
    "tokenized.set_format(type='torch', columns=['input_ids'])\n",
    "train_subset = tokenized[\"train\"].select(range(32000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "note: tying weights\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "#train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle = True) \n",
    "train_loader = DataLoader(tokenized[\"train\"], batch_size, shuffle = True) # one thing we want to look out for here is the shuffling seed.\n",
    "eval_loader = DataLoader(tokenized[\"test\"], 32, shuffle = False)\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "path = r'C:\\Users\\infin\\OneDrive\\Documents\\GitHub\\aimodels\\projects\\tooling'\n",
    "sys.path.insert(0, path)\n",
    "#from mixers import MixerModel, UpscalingEmbeddingsVectorizer, DownscalingLanguageModelHead, AttentionMixer, SeqConvMixer\n",
    "from mixers import MixerModel, AttentionMixer, NoNorm, ReZeroResidualBlock\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "model_dim, num_heads, num_layers = 768, 12, 12\n",
    "#model_dim, num_heads, num_layers = 256, 4, 4\n",
    "\n",
    "model = MixerModel(\n",
    "    model_size = model_dim,\n",
    "    num_layers = num_layers,\n",
    "    tokenizer = tokenizer,\n",
    "    max_seq_len = 512,\n",
    "    seq_mixer = (AttentionMixer, {\"num_heads\": num_heads}),\n",
    "    #norm = torch.nn.LayerNorm,\n",
    "    norm = NoNorm,\n",
    "    residual_block = ReZeroResidualBlock,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([0.], requires_grad=True)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder.layers[0].seq_block.norm.resweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "Training for 1 epochs starting from epoch 1; 6801 steps per epoch.\n",
      "Beginning epoch 1\n",
      "{'mode': 'train', 'epoch': 1, 'step': 1000, 'seconds': 890.5087018013, 'loss': 6.19204906642437, 'ppl': 488.84674072265625}\n",
      "{'mode': 'train', 'epoch': 1, 'step': 2000, 'seconds': 882.5054290294647, 'loss': 4.3682773955464365, 'ppl': 78.90760040283203}\n",
      "{'mode': 'train', 'epoch': 1, 'step': 3000, 'seconds': 898.9369604587555, 'loss': 3.87336490136385, 'ppl': 48.103981018066406}\n",
      "{'mode': 'train', 'epoch': 1, 'step': 4000, 'seconds': 867.2475390434265, 'loss': 3.5829718211591244, 'ppl': 35.980308532714844}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 18\u001b[0m\n\u001b[0;32m      6\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m      7\u001b[0m     model,\n\u001b[0;32m      8\u001b[0m     train_loader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m     gradient_accumulation_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m8\u001b[39m,\n\u001b[0;32m     15\u001b[0m )\n\u001b[0;32m     17\u001b[0m Trainer\u001b[38;5;241m.\u001b[39mclean_up_gpu()\n\u001b[1;32m---> 18\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\aimodels\\projects\\tooling\\train.py:402\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, epochs)\u001b[0m\n\u001b[0;32m    400\u001b[0m split_batch \u001b[38;5;241m=\u001b[39m _split_batch(train_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_loader\u001b[38;5;241m.\u001b[39mbatch_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgradient_accumulation_batch_size)\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m split_batch: \n\u001b[1;32m--> 402\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_loss(split, output, pad_token_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\OneDrive\\Documents\\GitHub\\aimodels\\projects\\tooling\\train.py:35\u001b[0m, in \u001b[0;36mdefault_forward_batch\u001b[1;34m(model, batch)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_forward_batch\u001b[39m(model, batch):\n\u001b[0;32m     34\u001b[0m     x, _ \u001b[38;5;241m=\u001b[39m unpack_batch(batch)\n\u001b[1;32m---> 35\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(x)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from train import Trainer, get_linear_schedule\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    tokenizer = tokenizer,\n",
    "    device = \"cuda\",\n",
    "    eval_loader = eval_loader,\n",
    "    log_every = 1000,\n",
    "    #schedule = get_linear_schedule(end_factor = 1.0), # you know what we didn't try?  using the normal scheduler.  we haven't actually tried constant warmup I don't think.\n",
    "    gradient_accumulation_batch_size = 8,\n",
    ")\n",
    "\n",
    "Trainer.clean_up_gpu()\n",
    "trainer.train(1) # alright, is this going to get down to 21 without warmup?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
