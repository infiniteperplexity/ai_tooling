{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright, let's take a break from beating my head against the divergence wall and instead implement some things.  First off, warmup and scheduling.\n",
    "# I think that's set, and it looks tantalizingly as if it's going to work.  It even seems like it might work for an unusually normal batch size.  Nope, it diverged.\n",
    "# Then I think I added gradient clipping properly.  I'll give it a try with the GPT-2 size model just in case that fixes it.\n",
    "# regardless of whether that works, the next two things are initialization strategies and norm ordering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial # There are going to be some things we want to initialize lazily to economize on resources and reuse constructor calls.\n",
    "# everything will use the same tokenizer\n",
    "from transformers import AutoTokenizer\n",
    "mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral, padding_side = \"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "267_477_061\n",
      "Max length: 236695, estimated tokens: 87_664\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf9efba9298749d8ad957fd1f28c5c13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/217608 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff0337634dce4b3fa024ba015f1ac377",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/24179 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Everything will use the same dataset and dataloaders\n",
    "repo = \"wikimedia/wikipedia\"\n",
    "import datasets\n",
    "ds = datasets.load_dataset(repo, \"20231101.simple\")\n",
    "def quick_estimate_tokens(ds, field=\"text\", chars_per_token=2.7):\n",
    "    tally = 0\n",
    "    max_len = 0\n",
    "    lengths = {}\n",
    "    for row in ds:\n",
    "        l = len(row[field])\n",
    "        tally += l\n",
    "        lengths[l] = lengths.get(l, 0) + 1\n",
    "        if l > max_len:\n",
    "            max_len = l\n",
    "\n",
    "    print(f'{int(tally):_}')\n",
    "    print(f'Max length: {max_len}, estimated tokens: {int(max_len / chars_per_token):_}')\n",
    "    lengths = list(lengths.items())\n",
    "    lengths.sort(reverse=True)\n",
    "    return int(tally/chars_per_token), lengths\n",
    "\n",
    "total, length = quick_estimate_tokens(ds['train'], field=\"text\")\n",
    "ds = ds[\"train\"].train_test_split(test_size=0.1)\n",
    "\n",
    "max_tokens = 512\n",
    "def batch_tokenize(batch):\n",
    "    return {\"input_ids\": tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=max_tokens).input_ids}\n",
    "\n",
    "tokenized = ds.map(batch_tokenize, batched=True, batch_size=1000)\n",
    "tokenized.set_format(type='torch', columns=['input_ids'])\n",
    "train_subset = tokenized[\"train\"].select(range(32000))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "note: tying weights\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "path = r'C:\\Users\\infin\\OneDrive\\Documents\\GitHub\\aimodels\\projects\\tooling'\n",
    "sys.path.insert(0, path)\n",
    "from mixer_norm_resid_order import MixerModel, UpscalingEmbeddingsVectorizer, DownscalingLanguageModelHead, AttentionMixer, SeqConvMixer, CyclingDecoderBackbone\n",
    "import torch\n",
    "from functools import partial\n",
    "\n",
    "model_dim, num_heads, num_layers = 128, 2, 2\n",
    "\n",
    "model = MixerModel(\n",
    "    model_size = model_dim,\n",
    "    num_layers = num_layers,\n",
    "    tokenizer = tokenizer,\n",
    "    max_seq_len = 512,\n",
    "    seq_mixer = (AttentionMixer, {\"num_heads\": num_heads}),\n",
    "    norm = torch.nn.LayerNorm,\n",
    "    decoder_backbone = CyclingDecoderBackbone,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MixerModel(\n",
       "  (vectorizer): EmbeddingAndPositionalVectorizer(\n",
       "    (embedding): Embedding(32000, 128)\n",
       "    (positional_embedding): Embedding(512, 128)\n",
       "  )\n",
       "  (embed_dropout): Dropout(p=0.0, inplace=False)\n",
       "  (embed_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (decoder): CyclingDecoderBackbone(\n",
       "    (Number of repeating layers: 2)\n",
       "    (seq_block): PreNormResidualBlock(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mixer): AttentionMixer(\n",
       "        (k_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (q_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (softmax): Softmax(dim=-1)\n",
       "        (v_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (out_proj): Linear(in_features=128, out_features=128, bias=False)\n",
       "        (dropout): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (ff_block): PreNormResidualBlock(\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (mixer): MLPMixer(\n",
       "        (fc1): Linear(in_features=128, out_features=512, bias=False)\n",
       "        (activation): GELU(approximate='none')\n",
       "        (fc2): Linear(in_features=512, out_features=128, bias=False)\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (head): CausalLanguageModelHead(\n",
       "    (fc): Linear(in_features=128, out_features=32000, bias=False)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (loss_fn): CrossEntropyLoss()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So I'm liking this quite a bit.  The question now is how to parameterize block ordering.  Then there's probably a related question of how to parameterize block reuse or cycling.\n",
    "# So starting with the former, one obvious way is to simply assume that pre-Norm and post-Norm are the only two possibilities, which is a more or less accurate assumption.  In that case I would probably default to something like \"pre_norm = True\".\n",
    "# My only real problem with that is that I suspect that generalizing the solution might solve other problems as well.\n",
    "# Let's come up with at least one alternative.  So, you would pass the entire residual block architecture as an argument.  Actually, a much better alternative is to have two subclasses and pass one of those.\n",
    "# Okay I'm pretty happy with that.  Now how about cycling and alternation?  So, let's be completely clear about what we're dealing with.\n",
    "# The most general way of looking at this, if we ignore the arbitrary distinction of \"layer\", is that we have alternating sequences of blocks that are not reused.\n",
    "# In ALBERT, we have alternating sequences of blocks that are reused.\n",
    "# In theory, we could have, say, four pairs of blocks that then get repeated three times.  We could also in theory have the opposite, three pairs of repeated blocks, followed by a different block repeated three times, and so on.\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/vision/zipball/v0.10.0\" to C:\\Users\\infin/.cache\\torch\\hub\\v0.10.0.zip\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\infin/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 77.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): BasicBlock(\n",
       "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): BasicBlock(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
