{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 256]) torch.Size([7, 256])\n",
      "torch.Size([3, 4, 7, 128]) torch.Size([3, 4, 7, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 7, 256])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Okay, it actually shouldn't be that hard to test whether this is getting the same results.  I don't know if I have it in my right now, though.\n",
    "import torch\n",
    "class RoPE:\n",
    "    _cache = {}\n",
    "    @classmethod\n",
    "    def _populate_cache(cls, dim, seq_len, device, cache_key):\n",
    "        dim, period = cache_key\n",
    "        inv_freq = 1.0 / (period ** (torch.arange(0, dim, 2, dtype=torch.int64).float().to(device) / dim))\n",
    "        t = torch.arange(seq_len, device = device, dtype = torch.int64).type_as(inv_freq)\n",
    "        with torch.autocast(device_type = device.type, enabled=False):\n",
    "            freqs = torch.outer(t, inv_freq)\n",
    "            emb = torch.cat((freqs, freqs), dim=-1)\n",
    "            cos = emb.cos().to(torch.get_default_dtype())\n",
    "            sin = emb.sin().to(torch.get_default_dtype())\n",
    "        cls._cache[cache_key] = (seq_len, cos, sin)\n",
    "        \n",
    "    @classmethod\n",
    "    def _get_cached_sin_con(cls, dim, seq_len, device, period = 10_000):\n",
    "        cache_key = (dim, period)\n",
    "        if cache_key not in cls._cache or seq_len > cls._cache[cache_key][0]:\n",
    "            cls._populate_cache(dim, seq_len, device, cache_key)\n",
    "        _, cos, sin = cls._cache[cache_key]\n",
    "        return cos[:seq_len].to(device), sin[:seq_len].to(device)\n",
    "\n",
    "    @classmethod\n",
    "    def embed(cls, x, period = 10_000, head_size = None):\n",
    "        seq_len = x.size(-2)\n",
    "        device = x.device\n",
    "        dim = head_size if head_size is not None else x.size(-1)\n",
    "        cos, sin = cls._get_cached_sin_con(dim, seq_len, device, period = period)\n",
    "        x1 = x[..., : x.shape[-1] // 2]\n",
    "        x2 = x[..., x.shape[-1] // 2 :]\n",
    "        rotated = torch.cat((-x2, x1), dim=-1)\n",
    "        embedded = (x * cos) + (rotated * sin)\n",
    "        return embedded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.modeling_llama import LlamaRotaryEmbedding\n",
    "from transformers.models.mistral.modeling_mistral import MistralRotaryEmbedding\n",
    "\n",
    "head_dim = 64\n",
    "num_heads = 4\n",
    "llama_rope = LlamaRotaryEmbedding(head_dim)\n",
    "mistral_rope = MistralRotaryEmbedding(head_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 7\n",
    "x = torch.randn(3, num_heads, seq_len, head_dim)\n",
    "cos_llama, sin_llama = llama_rope(x, seq_len = seq_len)\n",
    "cos_mistral, sin_mistral = mistral_rope(x, seq_len = seq_len)\n",
    "cos_me, sin_me = RoPE._get_cached_sin_con(head_dim, seq_len, x.device)\n",
    "## All seem to be equal; so far so good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 64]) torch.Size([7, 64])\n",
      "torch.Size([3, 4, 7, 32]) torch.Size([3, 4, 7, 32])\n",
      "torch.Size([7, 64]) torch.Size([7, 64])\n",
      "torch.Size([3, 4, 7, 32]) torch.Size([3, 4, 7, 32])\n"
     ]
    }
   ],
   "source": [
    "def rotate_half(x):\n",
    "    x1 = x[..., : x.shape[-1] // 2]\n",
    "    x2 = x[..., x.shape[-1] // 2 :]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n",
    "    print(cos.shape)\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    print(cos.shape)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "q_llama, k_llama = apply_rotary_pos_emb(x, x, cos_llama, sin_llama)\n",
    "#q_me, k_me = RoPE.embed(x), RoPE.embed(x)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
