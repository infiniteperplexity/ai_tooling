{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "path = r'C:\\Users\\infin\\OneDrive\\Documents\\GitHub\\aimodels\\projects\\tooling'\n",
    "sys.path.insert(0, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "repo = \"rahular/simple-wikipedia\"\n",
    "simple = datasets.load_dataset(repo)\n",
    "simple['train'][668959]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(342)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer(simple['train'][668959]['text']).input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer.decode(28705))\n",
    "print(tokenizer.decode(28750))\n",
    "print(tokenizer.decode(28725))\n",
    "print(tokenizer.decode([28725, 28705]))\n",
    "print(tokenizer(\",\"))\n",
    "print(tokenizer(\", \"))\n",
    "print(tokenizer(\"2 ,\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral, padding_side = \"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "repo = \"rahular/simple-wikipedia\"\n",
    "simple = datasets.load_dataset(repo)\n",
    "rows = [{'tokens': [tokenizer.bos_token_id]}]\n",
    "max_tokens = 512\n",
    "#line_break_token = tokenizer(\"\\n\").input_ids[-1]\n",
    "period_token = tokenizer(\".\").input_ids[-1]\n",
    "comma_token = tokenizer(\",\").input_ids[-1]\n",
    "for i, row in enumerate(simple['train']):\n",
    "    #print(f\"Processing row {i}.\")\n",
    "    last_row = rows[-1]\n",
    "    if len(last_row) > max_tokens:\n",
    "        print(\"that wasn't supposed to happen\")\n",
    "        break\n",
    "        #pass\n",
    "\n",
    "    tokens = tokenizer(row['text']).input_ids[1:]\n",
    "    if len(last_row['tokens']) + len(tokens) > max_tokens:\n",
    "        #print(f\"Closing out row with {len(last_row['tokens'])} tokens.\")\n",
    "        rows.append({'tokens': [tokenizer.bos_token_id]})\n",
    "        last_row = rows[-1]\n",
    "\n",
    "    splitting_tokens = [period_token, comma_token, 28723, 28725, 342] # seems to be some kind of numeric comma?\n",
    "    splitting_token = splitting_tokens.pop(0)\n",
    "\n",
    "    loops = 0\n",
    "    while len(tokens) > max_tokens: # look for ways to split the row\n",
    "        loops += 1\n",
    "        if loops > 100:\n",
    "            raise ValueError(\"Suspected infinite loop for row \" + str(i))\n",
    "            break\n",
    "\n",
    "        best_split = 0\n",
    "        for j, token in enumerate(tokens): # look for the splitting token\n",
    "            if token == splitting_token:\n",
    "                tentative_length = len(last_row['tokens']) + len(tokens[:j+1])\n",
    "                if tentative_length > max_tokens:\n",
    "                    if best_split == 0:\n",
    "                        rows.append({'tokens': [tokenizer.bos_token_id]}\n",
    "                        break # close out the row and restart the loop\n",
    "                    rows[-1]['tokens'].append(tokens[:best_split])\n",
    "                    tokens = tokens[best_split:]\n",
    "                    #print(f\"Closing out row with {len(last_row['tokens'])} tokens.\")\n",
    "                    rows.append({'tokens': [tokenizer.bos_token_id]})\n",
    "                    break # this will break out of the enumerate loop \n",
    "                else:\n",
    "                    best_split = j+1\n",
    "\n",
    "        if best_split > 0:\n",
    "            rows[-1]['tokens'] += tokens[:best_split]\n",
    "            tokens = tokens[best_split:]\n",
    "        else:\n",
    "            if len(splitting_tokens)>0:\n",
    "                splitting_token = splitting_tokens.pop(0)\n",
    "            else:\n",
    "                raise ValueError(\"Unable to find splitting token for row \" + str(i))\n",
    "                break\n",
    "\n",
    "    last_row['tokens'] += tokens\n",
    "\n",
    "                \n",
    "    # that works until we hit a very long row, then it breaks\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "ds = datasets.Dataset.from_pandas(df)\n",
    "ds # that takes less than three minuts to run and I'm guessing it's >90% accurate with the document breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tokenizer(\"2.\"))\n",
    "print(tokenizer(\"test.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral, padding_side = \"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "repo = \"rahular/simple-wikipedia\"\n",
    "simple = datasets.load_dataset(repo)\n",
    "rows = [{'tokens': [tokenizer.bos_token_id]}]\n",
    "max_tokens = 512\n",
    "split_tokens_ref = [842, 28723, 1200, 28725, 342] # both variants of period, both variants of comma, and pipe, so far\n",
    "for i, row in enumerate(simple['train']):\n",
    "    #print(f\"Processing row {i}.\")\n",
    "    last_row = rows[-1]\n",
    "    if len(last_row) > max_tokens: # if the current last row is too long, \n",
    "        raise ValueError(\"0: current row is too long at input row \" + str(i))\n",
    "\n",
    "    tokens = tokenizer(row['text']).input_ids[1:]\n",
    "    if len(last_row['tokens']) + len(tokens) > max_tokens:\n",
    "        #print(f\"Closing out row with {len(last_row['tokens'])} tokens.\")\n",
    "        rows.append({'tokens': [tokenizer.bos_token_id]})\n",
    "        last_row = rows[-1]\n",
    "\n",
    "    splitting_tokens = split_tokens_ref.copy()\n",
    "    splitting_token = splitting_tokens.pop(0)\n",
    "    loops = 0\n",
    "    while len(last_row) + len(tokens) > max_tokens: # look for ways to split the row\n",
    "        loops += 1\n",
    "        if loops > 100:\n",
    "            raise ValueError(\"Suspected infinite loop for row \" + str(i))\n",
    "            break\n",
    "\n",
    "        best_split = 0\n",
    "        for j, token in enumerate(tokens): # look for the splitting token\n",
    "            if token == splitting_token:\n",
    "                tentative_length = len(last_row['tokens']) + len(tokens[:j+1])\n",
    "                if tentative_length > max_tokens:\n",
    "                    if best_split == 0:\n",
    "                        if len(last_row['tokens']) > 512:\n",
    "                            raise ValueError(\"1: current row is too long at input row \" + str(i))\n",
    "                        rows.append({'tokens': [tokenizer.bos_token_id]})\n",
    "                        break # close out the row and restart the loop\n",
    "                    rows[-1]['tokens'].append(tokens[:best_split])\n",
    "                    tokens = tokens[best_split:]\n",
    "                    #print(f\"Closing out row with {len(last_row['tokens'])} tokens.\")\n",
    "                    rows.append({'tokens': [tokenizer.bos_token_id]})\n",
    "                    last_row = rows[-1]\n",
    "                    break # this will break out of the enumerate loop \n",
    "                else:\n",
    "                    best_split = j+1\n",
    "\n",
    "        if best_split > 0:\n",
    "            rows[-1]['tokens'] += tokens[:best_split]\n",
    "            if len(last_row['tokens']) > 512:\n",
    "                raise ValueError(\"2: current row is too long at input row \" + str(i))\n",
    "            tokens = tokens[best_split:]\n",
    "        else:\n",
    "            if len(splitting_tokens)>0:\n",
    "                splitting_token = splitting_tokens.pop(0)\n",
    "            else:\n",
    "                raise ValueError(\"Unable to find splitting token for row \" + str(i))\n",
    "\n",
    "    last_row['tokens'] += tokens\n",
    "    if len(last_row['tokens']) > 512:\n",
    "        raise ValueError(\"3: current row is too long at input row \" + str(i))\n",
    "\n",
    "                \n",
    "    # that works until we hit a very long row, then it breaks\n",
    "\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "ds = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (C:/Users/infin/.cache/huggingface/datasets/rahular___parquet/rahular--simple-wikipedia-e8322ace21be7483/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e1e065a5c534bfaabbb5610ce2a20e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing input row 0\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "mistral = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(mistral, padding_side = \"right\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "repo = \"rahular/simple-wikipedia\"\n",
    "simple = datasets.load_dataset(repo)\n",
    "\n",
    "def create_new_row():\n",
    "    return {'tokens': [tokenizer.bos_token_id]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macaroni Factory | Macaroni Factory | Madan-e Surameh | Madar Dokhtar | Madar Soleyman | Madavan | Madeh Banan | Madevan | Madkhun | Mah Farrokhan | Mah Kord | Mah Salari | Mahal Ahdas-e Sad Rudbal | Mahall ol Din | Mahalleh-ye Akbari | Maharak | Maharlu Kohneh | Maharlu Now | Maharzir | Mahdiyeh | Mah-e Firuzan | Mahjan | Mahjanabad | Mahlacheh | Mahmansaray Shomareh-ye Do | Mahmeleh | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad | Mahmudabad-e Do Dang | Mahmudabad-e Olya | Mahmudabad-e Olya | Mahmudabad-e Seh Dang | Mahmudabad-e Sofla | Mahmudabad-e Yek Dang | Main | Majdabad | Majdabad | Makuyeh | Mal Hajji | Malavan | Malay-e Anbar | Mal-e Ahmadi | Mal-e Mahmud | Malekabad | Malekabad | Malekabad | Malekabad | Malekabad | Malekabad | Malekabad | Malicheh Sheykh | Malicheh | Malyan | Mambalu | Mamu | Mangun | Manian | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad | Mansurabad-e Olya | Mansurabad-e Sofla | Mansuriyeh | Manuchehr Abbasi | Manuchehri | Maqsudabad | Maqsudabad | Maqsudabad | Maragallu | Marbuyeh | Margan | Marghdari-ye Hajji Hasan Kuh Bar | Marian | Marjanak | Marmeh | Marun | Marvashkan | Marvdasht | Marvdasht Agricultural Centre | Marzu | Masarm-e Olya | Masarm-e Sofla | Mashayekh | Mashayekh | Mashil Bandar-e Do | Mashil Bandar-e Yek | Mashtan | Masiri | Masumabad | Masumabad | Mayyu | Maz | Mazar | Mazayjan | Mazayjan | Mazekan | Mazraeh Seyyedi | Mazraeh | Mazraeh | Mazraeh | Mazraeh | Mazraeh-ye Ab Anjireh | Mazraeh-ye Ab Bid-e Pir Shams | Mazraeh-ye Abbasabad | Mazraeh-ye Abbasabad-e Morvarid | Mazraeh-ye Abdol Hoseyn Moradi | Mazraeh-ye Abdol Hoseyn Qobadi | Mazraeh-ye Abgarm | Mazraeh-ye Abu ol Qasem Sarferaz | Mazraeh-ye Afzalabad | Mazraeh-ye Ahmad Manuchehri | Mazraeh-ye Ahn Jahandideh | Mazraeh-ye Ahsham Molai | Mazraeh-ye Akbar Borhani | Mazraeh-ye Akbarabad | Mazraeh-ye Alafi | Mazraeh-ye Ali Akbar Hayati | Mazraeh-ye Ali Faqihi | Mazraeh-ye Ali Qoli Sadiqi | Mazraeh-ye Ali Reza Baharlu | Mazraeh-ye Ali Sirjani | Mazraeh-ye Aliabad | Mazraeh-ye Allah Qoli | Mazraeh-ye Amirabad | Mazraeh-ye Amrollah Panahpur | Mazraeh-ye Amrollah Rezai | Mazraeh-ye Aqa Hoseyn | Mazraeh-ye Arab | Mazraeh-ye Asadi | Mazraeh-ye Asadollah Zary | Mazraeh-ye Askar Sheybani | Mazraeh-ye Ayunabad | Mazraeh-ye Ayzadi | Mazraeh-ye Azad Khani | Mazraeh-ye Baba Abdollah | Mazraeh-ye Badabad | Mazraeh-ye Badakhoshan | Mazraeh-ye Bahram Pishgar | Mazraeh-ye Bandubast | Mazraeh-ye Bangalu Zardaval | Mazraeh-ye Baqr Sheybani | Mazraeh-ye Barikabad | Mazraeh-ye Beyglari | Mazraeh-ye Bidestan | Mazraeh-ye Bidu | Mazraeh-ye Chaghadu | Mazraeh-ye Chah Dozdan | Mazraeh-ye Chah Golabi | Mazraeh-ye Chah-e Shur-e Sofla | Mazraeh-ye Chal Kangari | Mazraeh-ye Charam Sefid | Mazraeh-ye Chari | Mazraeh-ye Chehel Moni | Mazraeh-ye Chengi Zard Chin | Mazraeh-ye Danesh | Mazraeh-ye Darishak | Mazraeh-ye Deh Dol | Mazraeh-ye Deh-e Dinari | Mazraeh-ye Deli-ye Rangak Rashidi | Mazraeh-ye Derazabad | Mazraeh-ye Diden Now | Mazraeh-ye Dowlatabad | Mazraeh-ye Dowlatabad | Mazraeh-ye Dozdakuiyeh | Mazraeh-ye Dulab-e Vasat | Mazraeh-ye Ebrahim Jahangiri | Mazraeh-ye Emam Qoli Azhaderi | Mazraeh-ye Emamzadeh Abdollah | Mazraeh-ye Emamzadeh Ahmad Ali Pichkan | Mazraeh-ye Emamzadeh Amrollah | Mazraeh-ye Emamzadeh Esmail | Mazraeh-ye Eslam Sadaqet | Mazraeh-ye Eslamabad | Mazraeh-ye Esmail Esmaili | Mazraeh-ye Fakhrabad | Mazraeh-ye Fakhrabi | Mazraeh-ye Falamarz Fuladi | Mazraeh-ye Fallahi | Mazraeh-ye Faqih | Mazraeh-ye Farhad Zarghami Koshkuli | Mazraeh-ye Farhadi | Mazraeh-ye Farj Flamarzy | Mazraeh-ye Farjineh | Mazraeh-ye Fathabad | Mazraeh-ye Fereydun Puya | Mazraeh-ye Gachi | Mazraeh-ye Galak | Mazraeh-ye Gholam Hoseyn Khan Masumi | Mazraeh-ye Gholam Mollai | Mazraeh-ye Gholam Reza Fuladiyan | Mazraeh-ye Gurki | Mazraeh-ye Gurki Malekzadeh | Mazraeh-ye Hadi Keshtkaran | Mazraeh-ye Hajj Khani | Mazraeh-ye Hajji Allahdad Moqbeli | Mazraeh-ye Hajji Baba Rastagu | Mazraeh-ye Hajji Baha ol Din | Mazraeh-ye Hajji Karami | Mazraeh-ye Hajjiabad | Mazraeh-ye Hajjiabad | Mazraeh-ye Hajjiabad | Mazraeh-ye Hamataj Bahbahani | Mazraeh-ye Hamidabad | Mazraeh-ye Hammam ol Din | Mazraeh-ye Harar-e Sefid | Mazraeh-ye Harunak | Mazraeh-ye Hasan Ali Zarghami va Shorkay | Mazraeh-ye Hasan Zemani | Mazraeh-ye Hasanabad-e Shul | Mazraeh-ye Henduyeh | Mazraeh-ye Heyati | Mazraeh-ye Hoseyn Taqizadeh | Mazraeh-ye Hoseynabad | Mazraeh-ye Hoseynabad | Mazraeh-ye Hoseynabad | Mazraeh-ye Huzang | Mazraeh-ye Isa Ahmadi | Mazraeh-ye Jafarabad | Mazraeh-ye Jahangir Namdary | Mazraeh-ye Jalali | Mazraeh-ye Janbaz Salehi | Mazraeh-ye Jareh | Mazraeh-ye Javad Najafi | Mazraeh-ye Javadastavar | Mazraeh-ye Jowzar | Mazraeh-ye Kalikhani | Mazraeh-ye Karimi | Mazraeh-ye Karmani | Mazraeh-ye Karmshah Karmi | Mazraeh-ye Kasheh Tanqur | Mazraeh-ye Katu | Mazraeh-ye Kazem Zar | Mazraeh-ye Khalifabad | Mazraeh-ye Kheyrabad | Mazraeh-ye Khodadad Sheybani | Mazraeh-ye Khukand | Mazraeh-ye Kurshabad | Mazraeh-ye Kushk | Mazraeh-ye Lahrasb | Mazraeh-ye Lard Khun | Mazraeh-ye Las | Mazraeh-ye Lohrasb Shahbazi | Mazraeh-ye Madan | Mazraeh-ye Madrasi | Mazraeh-ye Maghilan | Mazraeh-ye Mahbati | Mazraeh-ye Mahmud Parmayeh | Mazraeh-ye Mahmudabad | Mazraeh-ye Mallu | Mazraeh-ye Manuchehr Najafi | Mazraeh-ye Masud Barani va Shorkadh | Mazraeh-ye Mehdiabad | Mazraeh-ye Mehdiabad-e Now | Mazraeh-ye Mehdiabad-e Now | Mazraeh-ye Mehrabi | Mazraeh-ye Miri | Mazraeh-ye Mohammad Abbasi | Mazraeh-ye Mohammad Ali Asgari | Mazraeh-ye Mohammad Ali Gerashi | Mazraeh-ye Mohammad Ali Zafarabadi | Mazraeh-ye Mohammad Hoseyn Hoseyni | Mazraeh-ye Mohammad Karam Sharifi | Mazraeh-ye Mohammad Karim Nury | Mazraeh-ye Mohammad Karimi | Mazraeh-ye Mohammad Masumi | Mazraeh-ye Mohammad Mazafarian | Mazraeh-ye Mohammad Mazidi | Mazraeh-ye Mohammad Nabi Arjomand | Mazraeh-ye Mohammad Nur Dashti | Mazraeh-ye Mohammad Pak Shir | Mazraeh-ye Mohammad Qobadi | Mazraeh-ye Mohammad Qoli Bahramian | Mazraeh-ye Mohammad Sadeq Eqbal | Mazraeh-ye Mohammadabad | Mazraeh-ye Mohammady | Mazraeh-ye Molla Hadi | Mazraeh-ye Morad Ali Pay Moradi | Mazraeh-ye Morad Heydari | Mazraeh-ye Moruji | Mazraeh-ye Murdi | Mazraeh-ye Najafabad | Mazraeh-ye Najafabad-e Bazud | Mazraeh-ye Naser Esmaili | Mazraeh-ye Nasratollah Najafi | Mazraeh-ye Nasrollah Jafary | Mazraeh-ye Nazem Sur | Mazraeh-ye Nematollah Rah Pima | Mazraeh-ye Nosrati | Mazraeh-ye Nuli | Mazraeh-ye Pahn | Mazraeh-ye Pahna | Mazraeh-ye Palangi | Mazraeh-ye Panj Ali Karimi | Mazraeh-ye Panj Chah | Mazraeh-ye Parvarsh Mahi | Mazraeh-ye Pater Chakhar | Mazraeh-ye Pir Badam | Mazraeh-ye Posht Kak | Mazraeh-ye Puram Shahadi Anhas | Mazraeh-ye Qanbarabad | Mazraeh-ye Qatarband | Mazraeh-ye Qeytas Najarzadeh | Mazraeh-ye Qomsur | Mazraeh-ye Rah Javian | Mazraeh-ye Rahim Alikam ol Salam | Mazraeh-ye Rahman Fuladi | Mazraeh-ye Rahmatabad | Mazraeh-ye Rajab Ali Lahrasabi | Mazraeh-ye Rajab Zari | Mazraeh-ye Ramazan Baradbar | Mazraeh-ye Rast Bud | Mazraeh-ye Razak | Mazraeh-ye Reza Mohsul | Mazraeh-ye Sadeqabad | Mazraeh-ye Sadeqiyeh | Mazraeh-ye Saidabad | Mazraeh-ye Sang Bar | Mazraeh-ye Sar Bisheh | Mazraeh-ye Sarhadi | Mazraeh-ye Sarhang Dabiri | Mazraeh-ye Seh Chah Kuh Sorkh | Mazraeh-ye Seh Qanat | Mazraeh-ye Seyf ol Din | Mazraeh-ye Shah Ali | Mazraeh-ye Shah Mowr | Mazraeh-ye Shah Reza Karmi va Shork | Mazraeh-ye Shah Taj | Mazraeh-ye Shahbaz Karami | Mazraeh-ye Shahbazi | Mazraeh-ye Shahid Dast Gheyb | Mazraeh-ye Shakrollah Shekari | Mazraeh-ye Shariati | Mazraeh-ye Shekaft Zun | Mazraeh-ye Shimak Shur | Mazraeh-ye Shirvani va Shorka | Mazraeh-ye Shurab | Mazraeh-ye Shuru Hajjiabad | Mazraeh-ye Shuru Hoseyni | Mazraeh-ye Siah Daneh | Mazraeh-ye Takht Tavus Homayun | Mazraeh-ye Taksh | Mazraeh-ye Talami | Mazraeh-ye Tall Goreh | Mazraeh-ye Tall Roshtan | Mazraeh-ye Tall Sangari | Mazraeh-ye Tall Sangbary | Mazraeh-ye Tang Firuzi | Mazraeh-ye Tareh Kad Shomareh-ye Do | Mazraeh-ye Tareh Kad Shomareh-ye Yek | Mazraeh-ye Tarvij Gandam | Mazraeh-ye Tayfeh Galeh Zan | Mazraeh-ye Tayifeh Farhadlu | Mazraeh-ye Tireh Bahi | Mazraeh-ye Tulayi | Mazraeh-ye Vajdani | Mazraeh-ye Vali Mohammad Qohrmani | Mazraeh-ye Yadollah Jafari | Mazraeh-ye Yunes Hamidi | Mazraeh-ye Zaki Khan | Mazraeh-ye Zarghami | Mazraeh-ye Zeman Rostami | Mazraeh-ye Zeynal Mardani | Mazraeh-ye Zeytunak | Mazru | Mehbudi-ye Olya | Mehbudi-ye Sofla | Mehdi Residential Housing | Mehdiabad | Mehdiabad | Mehdiabad | Mehdiabad | Mehdiabad | Mehdiabad | Mehdiabad | Mehdiabad | Mehkuyeh-ye Olya | Mehkuyeh-ye Sofla | Mehmanabad | Mehr Ali Khan-e Bon Rud | Mehrabad | Mehrabad | Mehrabad | Mehrabad | Mehrabad | Mehrabad | Mehrabad | Mehrabad-e Mandegari | Mehrabad-e Mazidi | Mehrenjan | Mehrenjan | Mehrian | Mehrian | Meleh Galeh | Melk Melk | Melleh Kangun | Melleh Khik Andeh | Menaruyeh | Menguiyeh | Meshkan | Meydan | Meydanak | Meygoli | Meymand | Mezijan | Mian Gowd | Mian Qaleh | Mian Rud | Miandeh | Mianeh | Mianeh-ye Jenjan | Mianrud | Midjan | Mij | Milatun | Miluyeh | Mina | Mir Hasani | Mir Kheyrollah | Mir Maleki | Mirchakak | Mirchaki | Mirisah | Mirzaali | Mirzamohammadi-ye Bala | Mirzamohammadi-ye Pain | Mishan-e Olya | Mishan-e Sofla | Miyan Deh | Miyaneh-ye Olya | Miyaneh-ye Sofla | Mobarakabad | Mobarakabad | Mobarakabad | Mobarakabad | Mobarakeh | Mobarakeh | Moezzabad-e Gurgir | Moezzabad-e Jaberi | Mogharrab-e Do | Mohammad Qasemi | Mohammad Zeyna | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad | Mohammadabad-e Sofla | Mohammad-e Olya | Mohemmabad | Mohit Ab | Mohr | Mohsenabad | Moinabad | Moinabad | Mokh Sukhteh | Mokhtarabad-e Bon Rud | Molaim Soap Factory | Molani | Molay-ye Sefid | Molk-e Ali | Molla Arreh | Molla Balut | Molla Hasani | Mollai | Monj-e Olya | Monj-e Sofla | Moqaberi | Moqarrab-e Yek | Moqbelabad | Moradabad | Moradabad | Moradabad-e Kolah Siah | Moraskhun-e Olya | Moraskhun-e Sofla | Morghak | Morghan | Morgh-e Bozorg | Morgh-e Kuchak | Moridan | Morshedi | Morz | Morzian | Moshkan | Moshtagan | Moslemabad | Mowmenabad | Mowr Deraz | Mowruzeh | Mowzar | Mozaffarabad | Mozaffarabad | Mozaffari | Mozaffari | Mozaffari | Multul | Mur Pahn | Muraki | Murchagi | Murdak | Murdak | Murd-e Susani | Murderaz | Murdestan | Murdestan | Murdi | Mur-e Deraz-e Bon Rud | Murekord | Murgah-e Baba Monir | Murjan | Murj-e Shahrak | Murkash | Murmir | Mushkan | Musqan\n",
      "[1, 5325, 5214, 28710, 24264, 342, 5325, 5214, 28710, 24264, 342, 5311, 276, 28733, 28706, 6021, 433, 28716, 342, 5311, 283, 384, 493, 407, 283, 342, 5311, 283, 318, 1254, 21665, 342, 5311, 18872, 342, 16970, 28716, 14859, 276, 342, 16970, 7675, 342, 5311, 15887, 370, 342, 10947, 8549, 27644, 7645, 342, 10947, 524, 556, 342, 10947, 4902, 1900, 342, 10947, 282, 330, 8385, 293, 28733, 28706, 16750, 15010, 6323, 342, 10947, 455, 10370, 20325, 342, 10947, 4504, 28716, 28733, 7187, 9532, 1822, 28710, 342, 10947, 283, 491, 342, 10947, 1977, 28718, 524, 1371, 12667, 342, 10947, 1977, 28718, 2961, 342, 10947, 21896, 361, 342, 10947, 5605, 7187, 28716, 342, 10947, 28733, 28706, 401, 361, 3533, 276, 342, 10947, 12517, 342, 10947, 12517, 20882, 342, 351, 7010, 1344, 28716, 342, 10947, 20661, 283, 339, 1295, 300, 492, 28716, 28733, 7187, 2378, 342, 10947, 1127, 291, 28716, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 342, 10947, 28719, 554, 20882, 28733, 28706, 2378, 384, 602, 342, 10947, 28719, 554, 20882, 28733, 28706, 451, 346, 28708, 342, 10947, 28719, 554, 20882, 28733, 28706, 451, 346, 28708, 342, 10947, 28719, 554, 20882, 28733, 28706, 1091, 28716, 384, 602, 342, 10947, 28719, 554, 20882, 28733, 28706, 21918, 2220, 342, 10947, 28719, 554, 20882, 28733, 28706, 627, 950, 384, 602, 342, 8105, 342, 16745, 28715, 20882, 342, 16745, 28715, 20882, 342, 20046, 4533, 10069, 342, 4380, 382, 1150, 3632, 342, 4380, 18872, 342, 4380, 339, 28733, 28706, 1094, 1822, 342, 4380, 28733, 28706, 10103, 28719, 12035, 342, 4380, 28733, 28706, 10947, 28719, 554, 342, 23304, 28729, 20882, 342, 23304, 28729, 20882, 342, 23304, 28729, 20882, 342, 23304, 28729, 20882, 342, 23304, 28729, 20882, 342, 23304, 28729, 20882, 342, 23304, 28729, 20882, 342, 4380, 5680, 28716, 985, 28724, 15887, 342, 4380, 5680, 28716, 342, 4380, 8455, 342, 351, 1379, 5107, 342, 28388, 28718, 342, 351, 602, 370, 342, 2213, 753, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 342, 24800, 324, 20882, 28733, 28706, 451, 346, 28708, 342, 24800, 324, 20882, 28733, 28706, 21918, 2220, 342, 24800, 6395, 7187, 28716, 342, 2213, 28718, 631, 2176, 15859, 8608, 342, 2213, 28718, 631, 28716, 373, 342, 6068, 25065, 554, 20882, 342, 6068, 25065, 554, 20882, 342, 6068, 25065, 554, 20882, 342, 1471, 357, 455, 28718, 342, 1471, 28726, 4533, 10069, 342, 10149, 276, 342, 1471, 591, 28715, 1900, 28733, 7187, 382, 1150, 3632, 10981, 276, 524, 8884, 3011, 342, 27117, 342, 1471, 12517, 491, 342, 1471, 1127, 28716, 342, 1471, 370, 342, 1471, 28728, 1029, 9763, 342, 1471, 17249, 293, 407, 342, 1471, 17249, 293, 407, 23837, 1890, 11304, 342, 1471, 13581, 342, 9183, 1785, 28733, 28706, 451, 346, 28708, 342, 9183, 1785, 28733, 28706, 21918, 2220, 342, 351, 1029, 339, 950, 28716, 342, 351, 1029, 339, 950, 28716, 342, 351, 1029, 309, 10521, 283, 28733, 28706, 2378, 342, 351, 1029, 309, 10521, 283, 28733, 28706, 627, 950, 342, 9183, 407, 276, 342, 9183, 16105, 342, 9183, 383, 20882, 342, 9183, 383, 20882, 342, 2246, 28724, 28718, 342, 25131, 342, 351, 21364, 342, 25131, 339, 12517, 342, 25131, 339, 12517, 342, 25131, 950, 276, 342, 25131, 520, 10069, 1091, 8772, 14548, 342, 25131, 520, 10069, 342, 25131, 520, 10069, 342, 25131, 520, 10069, 342, 25131, 520, 10069, 342, 25131, 520, 10069, 28733, 7187, 2484, 1094, 28768, 536, 28716, 342, 25131, 520, 10069, 28733, 7187, 2484, 365, 313, 28733, 28706, 19346, 1295, 5322, 342, 25131, 520, 10069, 28733, 7187, 15859, 293, 20882, 342, 25131, 520, 10069, 28733, 7187, 15859, 293, 20882, 28733, 28706, 4151, 1122, 313, 342, 25131, 520, 10069, 28733, 7187, 18386, 328, 382, 645, 1035, 4151, 12035, 342, 25131, 520, 10069, 28733, 7187, 18386, 328, 382, 645, 1035, 1186, 598, 12035, 342, 25131, 520, 10069, 28733, 7187, 2484, 28721, 1785, 342, 25131, 520, 10069, 28733, 7187, 24712, 10370, 1186, 293, 366, 7663, 642, 941, 342, 25131, 520, 10069, 28733, 7187, 8786, 28764, 282, 20882, 342, 25131, 520, 10069, 28733, 7187, 10103, 13179, 2213, 28718, 631, 28716, 373, 342, 25131, 520, 10069, 28733, 7187, 10103, 28711, 4096, 391, 547, 28716, 342, 25131, 520, 10069, 28733, 7187, 10103, 811, 314, 16387, 1585, 342, 25131, 520, 10069, 28733, 7187, 9532, 1822, 9411, 28716, 4499, 342, 25131, 520, 10069, 28733, 7187, 9532, 1822, 20882, 342, 25131, 520, 10069, 28733, 7187, 976, 27312, 342, 25131, 520, 10069, 28733, 7187, 15284, 9532, 1822, 11411, 3986, 342, 25131, 520, 10069, 28733, 7187, 15284, 12594, 28775, 4371, 28710, 342, 25131, 520, 10069, 28733, 7187, 15284, 1186, 7320, 318, 12035, 28775, 28710, 342, 25131, 520, 10069, 28733, 7187, 15284, 1298, 2166, 20057, 1977, 28718, 342, 25131, 520, 10069, 28733, 7187, 15284, 8351, 28768, 4499, 342, 25131, 520, 10069, 28733, 7187, 15284, 20882, 342, 25131, 520, 10069, 28733, 7187, 1682, 912, 1186, 7320, 342, 25131, 520, 10069, 28733, 7187, 2740, 361, 20882, 342, 25131, 520, 10069, 28733, 7187, 2740, 1584, 912, 7687, 912, 14692, 342, 25131, 520, 10069, 28733, 7187, 2740, 1584, 912, 1298, 28764, 1585, 342, 25131, 520, 10069, 28733, 7187, 330, 19259, 382, 645, 1035, 342, 25131, 520, 10069, 28733, 7187, 9111, 342, 25131, 520, 10069, 28733, 7187, 1136, 12035, 342, 25131, 520, 10069, 28733, 7187, 1136, 316, 793, 912, 1054, 628, 342, 25131, 520, 10069, 28733, 7187, 1136, 13600, 985, 28724, 3627, 28710, 342, 25131, 520, 10069, 28733, 7187, 17161, 370, 20882, 342, 25131, 520, 10069, 28733, 7187, 17161, 28764, 12035, 342, 25131, 520, 10069, 28733, 7187, 7393, 316, 11729, 4499, 342, 25131, 520, 10069, 28733, 7187, 365, 5544, 18386, 793, 912, 342, 25131, 520, 10069, 28733, 7187, 9734, 20882, 342, 25131, 520, 10069, 28733, 7187, 9734, 17212, 6643, 276, 342, 25131, 520, 10069, 28733, 7187, 20057, 3212, 367, 789, 4749, 342, 25131, 520, 10069, 28733, 7187, 10521, 437, 529, 342, 25131, 520, 10069, 28733, 7187, 13261, 5107, 1054, 488, 12239, 342, 25131, 520, 10069, 28733, 7187, 9236, 28775, 28712, 985, 28724, 3627, 28710, 342, 25131, 520, 10069, 28733, 7187, 3011, 849, 20882, 342, 25131, 520, 10069, 28733, 7187, 16015, 1727, 1900, 342, 25131, 520, 10069, 28733, 7187, 365, 313, 374, 276, 342, 25131, 520, 10069, 28733, 7187, 365, 313, 28718, 342, 25131, 520, 10069, 28733, 7187, 689, 357, 16449, 28718, 342, 25131, 520, 10069, 28733, 7187, 689, 912, 2378, 13335, 276, 342, 25131, 520, 10069, 28733, 7187, 689, 912, 27085, 16075, 342, 25131, 520, 10069, 28733, 7187, 689, 912, 28733, 28706, 1295, 324, 28733, 28706, 21918, 2220, 342, 25131, 520, 10069, 28733, 7187, 689, 282, 524, 602, 1900, 342, 25131, 520, 10069, 28733, 7187, 689, 762, 318, 797, 313, 342, 25131, 520, 10069, 28733, 7187, 689, 1900, 342, 25131, 520, 10069, 28733, 7187, 4537, 2805, 3217, 28710, 342, 25131, 520, 10069, 28733, 7187, 17038, 5330, 1054, 488, 689, 262, 342, 25131, 520, 10069, 28733, 7187, 4294, 9366, 342, 25131, 520, 10069, 28733, 7187, 8220, 789, 491, 342, 25131, 520, 10069, 28733, 7187, 1343, 28716, 15052, 342, 25131, 520, 10069, 28733, 7187, 1343, 28716, 28733, 28706, 20325, 1900, 342, 25131, 520, 10069, 28733, 7187, 5526, 28710, 28733, 7187, 24237, 491, 399, 1029, 10064, 342, 25131, 520, 10069, 28733, 7187, 4823, 941, 20882, 342, 25131, 520, 10069, 28733, 7187, 384, 6667, 2961, 342, 25131, 520, 10069, 28733, 7187, 20271, 7387, 20882, 342, 25131, 520, 10069, 28733, 7187, 20271, 7387, 20882, 342, 25131, 520, 10069, 28733, 7187, 2378, 13335, 491, 1724, 7187, 28716, 342, 25131, 520, 10069, 28733, 7187, 384, 353, 375, 28733, 28706, 19286, 270, 342, 25131, 520, 10069, 28733, 7187, 413, 10675, 24669, 4096, 602, 16105, 342, 25131, 520, 10069, 28733, 7187, 2929, 314, 1186, 7320, 7393, 28716, 4928, 28710, 342, 25131, 520, 10069, 28733, 7187, 2929, 314, 28764, 770, 28716, 18386, 793, 912, 342, 25131, 520, 10069, 28733, 7187, 2929, 314, 28764, 770, 28716, 10103, 13179, 15284, 367, 539, 9763, 342, 25131, 520, 10069, 28733, 7187, 2929, 314, 28764, 770, 28716, 2740, 1584, 912, 342, 25131, 520, 10069, 28733, 7187, 2929, 314, 28764, 770, 28716, 413, 2762, 614, 342, 25131, 520, 10069, 28733, 7187, 413, 2181, 314, 318, 1540, 28775, 299, 342, 25131, 520, 10069, 28733, 7187, 413, 2181, 314, 20882, 342, 25131, 520, 10069, 28733, 7187, 413, 2762, 614, 413, 2762, 614, 28710, 342, 25131, 520, 10069, 28733, 7187, 401, 491, 2176, 20882, 342, 25131, 520, 10069, 28733, 7187, 401, 491, 2176, 16075, 342, 25131, 520, 10069, 28733, 7187, 16651, 11369, 28764, 401, 353, 12035, 342, 25131, 520, 10069, 28733, 7187, 12168, 912, 28710, 342, 25131, 520, 10069, 28733, 7187, 12594, 28775, 4371, 342, 25131, 520, 10069, 28733, 7187, 8549, 16449, 1054, 283, 591, 6449, 524, 6643, 28729, 11815, 342, 25131, 520, 10069, 28733, 7187, 8549, 28716, 12035, 342, 25131, 520, 10069, 28733, 7187, 8549, 28768, 2494, 11369, 2140, 342, 25131, 520, 10069, 28733, 7187, 8549, 28768, 473, 28716, 342, 25131, 520, 10069, 28733, 7187, 401, 498, 20882, 342, 25131, 520, 10069, 28733, 7187, 401, 397, 3389, 370, 367, 4533, 28708, 342, 25131, 520, 10069, 28733, 7187, 420, 23827, 342, 25131, 520, 10069, 28733, 7187, 6639, 491, 342, 25131, 520, 10069, 28733, 7187, 420, 5236, 314, 382, 645, 1035, 18595, 9183, 12965, 342, 25131, 520, 10069, 28733, 7187, 420, 5236, 314, 351, 793, 1585, 342, 25131, 520, 10069, 28733, 7187, 420, 5236, 314, 1298, 2166, 401, 353, 12035, 8455, 342, 25131, 520, 10069, 28733, 7187, 420, 324, 3430, 342, 25131, 520, 10069, 28733, 7187, 420, 324, 3430, 23304, 28729, 28764, 770, 28716, 342, 25131, 520, 10069, 28733, 7187, 11238, 28710, 524, 274, 407, 13600, 276, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 28768, 11729, 4499, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 1682, 912, 28715, 316, 6885, 28775, 7244, 28710, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 365, 5544, 399, 529, 12736, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 365, 12980, 10370, 20325, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 524, 762, 28710, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 20882, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 20882, 342, 25131, 520, 10069, 28733, 7187, 382, 1150, 3632, 20882, 342, 25131, 520, 10069, 28733, 7187, 5058, 563, 28768, 20057, 28726, 912, 4499, 342, 25131, 520, 10069, 28733, 7187, 5058, 313, 20882, 342, 25131, 520, 10069, 28733, 7187, 5058, 28719, 314, 10370, 20325, 342, 25131, 520, 10069, 28733, 7187, 3407, 283, 28733, 28706, 318, 797, 313, 342, 25131, 520, 10069, 28733, 7187, 3407, 370, 491, 342, 25131, 520, 10069, 28733, 7187, 10981, 276, 15284, 1054, 283, 591, 6449, 4536, 1295, 580, 339, 342, 25131, 520, 10069, 28733, 7187, 10981, 276, 1054, 366, 4499, 342, 25131, 520, 10069, 28733, 7187, 10981, 276, 20882, 28733, 28706, 1295, 353, 342, 25131, 520, 10069, 28733, 7187, 18588, 4533, 10069, 342, 25131, 520, 10069, 28733, 7187, 17162, 3986, 342, 25131, 520, 10069, 28733, 7187, 382, 645, 1035, 14247, 28775, 463, 770, 28716, 342, 25131, 520, 10069, 28733, 7187, 382, 645, 1035, 20882, 342, 25131, 520, 10069, 28733, 7187, 382, 645, 1035, 20882, 342, 25131, 520, 10069, 28733, 7187, 382, 645, 1035, 20882, 342, 25131, 520, 10069, 28733, 7187, 382, 3533, 602, 342, 25131, 520, 10069, 28733, 7187, 17578, 10103, 28719, 12035, 342, 25131, 520, 10069, 28733, 7187, 475, 2015, 283, 20882, 342, 25131, 520, 10069, 28733, 7187, 4096, 602, 361, 16908, 28715, 628, 342, 25131, 520, 10069, 28733, 7187, 475, 282, 4827, 342, 25131, 520, 10069, 28733, 7187, 2997, 28726, 941, 19338, 5365, 342, 25131, 520, 10069, 28733, 7187, 475, 492, 28716, 342, 25131, 520, 10069, 28733, 7187, 475, 494, 316, 26293, 27312, 342, 25131, 520, 10069, 28733, 7187, 475, 494, 316, 529, 494, 283, 342, 25131, 520, 10069, 28733, 7187, 475, 336, 28764, 283, 342, 25131, 520, 10069, 28733, 7187, 12117, 26276, 4499, 342, 25131, 520, 10069, 28733, 7187, 7775, 13840, 342, 25131, 520, 10069, 28733, 7187, 7775, 1294, 28710, 342, 25131, 520, 10069, 28733, 7187, 524, 1785, 811, 912, 524, 1785, 28710, 342, 25131, 520, 10069, 28733, 7187, 22965, 265, 28716, 17157, 28775, 324, 342, 25131, 520, 10069, 28733, 7187, 14294, 28718, 342, 25131, 520, 10069, 28733, 7187, 16359, 366, 1054, 283, 342, 25131, 520, 10069, 28733, 7187, 524, 6019, 335, 20882, 342, 25131, 520, 10069, 28733, 7187, 524, 265, 4369, 20882, 342, 25131, 520, 10069, 28733, 7187, 11729, 350, 316, 316, 985, 28724, 3627, 28710, 342, 25131, 520, 10069, 28733, 7187, 11729, 2950, 391, 342, 25131, 520, 10069, 28733, 7187, 15740, 811, 20882, 342, 25131, 520, 10069, 28733, 7187, 524, 1426, 28729, 342, 25131, 520, 10069, 28733, 7187, 393, 912, 5895, 28726, 342, 25131, 520, 10069, 28733, 7187, 393, 488, 11729, 370, 342, 25131, 520, 10069, 28733, 7187, 9134, 342, 25131, 520, 10069, 28733, 7187, 393, 1371, 5895, 28726, 23452, 28726, 19597, 342, 25131, 520, 10069, 28733, 7187, 5311, 276, 342, 25131, 520, 10069, 28733, 7187, 5311, 5895, 28710, 342, 25131, 520, 10069, 28733, 7187, 4642, 28716, 309, 276, 342, 25131, 520, 10069, 28733, 7187, 10947, 28726, 3986, 342, 25131, 520, 10069, 28733, 7187, 10947, 28719, 554, 367, 1785, 339, 10069, 342, 25131, 520, 10069, 28733, 7187, 10947, 28719, 554, 20882, 342, 25131, 520, 10069, 28733, 7187, 20098, 28718, 342, 25131, 520, 10069, 28733, 7187, 2213, 28718, 631, 2176, 26293, 27312, 342, 25131, 520, 10069, 28733, 7187, 9183, 554, 3011, 4499, 4536, 1295, 580, 316, 28716, 342, 25131, 520, 10069, 28733, 7187, 2597, 28716, 5605, 20882, 342, 25131, 520, 10069, 28733, 7187, 2597, 28716, 5605, 20882, 28733, 28706, 2961, 342, 25131, 520, 10069, 28733, 7187, 2597, 28716, 5605, 20882, 28733, 28706, 2961, 342, 25131, 520, 10069, 28733, 7187, 2597, 2176, 16075, 342, 25131, 520, 10069, 28733, 7187, 9154, 28710, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 15859, 8608, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 15284, 1136, 28721, 1900, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 15284, 7801, 19915, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 15284, 1054, 2015, 283, 375, 12035, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 382, 645, 1035, 382, 645, 1035, 28710, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 524, 762, 20071, 10273, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 7775, 321, 418, 2272, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 7775, 13840, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 9183, 12965, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 25131, 2015, 9459, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 25131, 10064, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 418, 16075, 1010, 28768, 300, 391, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 23381, 7029, 407, 28710, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 12044, 25959, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 1186, 598, 12035, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 1186, 7320, 20057, 3212, 753, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 318, 770, 28775, 9692, 6323, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 316, 20882, 342, 25131, 520, 10069, 28733, 7187, 13610, 6461, 4425, 342, 25131, 520, 10069, 28733, 7187, 351, 793, 28708, 11238, 28710, 342, 25131, 520, 10069, 28733, 7187, 4151, 316, 15284, 9260, 4151, 12035, 342, 25131, 520, 10069, 28733, 7187, 4151, 316, 650, 3389, 1900, 342, 25131, 520, 10069, 28733, 7187, 4151, 28718, 3632, 342, 25131, 520, 10069, 28733, 7187, 7423, 5605, 342, 25131, 520, 10069, 28733, 7187, 26293, 2015, 20882, 342, 25131, 520, 10069, 28733, 7187, 26293, 2015, 20882, 28733, 28706, 365, 941, 554, 342, 25131, 520, 10069, 28733, 7187, 418, 14068, 413, 2762, 614, 28710, 342, 25131, 520, 10069, 28733, 7187, 22012, 5308, 793, 912, 26293, 27312, 342, 25131, 520, 10069, 28733, 7187, 22012, 1584, 912, 475, 2015, 628, 342, 25131, 520, 10069, 28733, 7187, 10283, 366, 6021, 342, 25131, 520, 10069, 28733, 7187, 418, 16633, 793, 912, 20398, 367, 4250, 342, 25131, 520, 10069, 28733, 7187, 418, 385, 28712, 3986, 342, 25131, 520, 10069, 28733, 7187, 418, 11815, 342, 25131, 520, 10069, 28733, 7187, 367, 8745, 342, 25131, 520, 10069, 28733, 7187, 367, 912, 1520, 342, 25131, 520, 10069, 28733, 7187, 4187, 602, 28710, 342, 25131, 520, 10069, 28733, 7187, 7687, 28768, 15284, 7775, 13840, 342, 25131, 520, 10069, 28733, 7187, 7687, 28768, 689, 912, 342, 25131, 520, 10069, 28733, 7187, 2316, 1122, 811, 10947, 28710, 342, 25131, 520, 10069, 28733, 7187, 367, 795, 689, 491, 10327, 342, 25131, 520, 10069, 28733, 7187, 19346, 9734, 314, 342, 25131, 520, 10069, 28733, 7187, 8202, 407, 524, 491, 342, 25131, 520, 10069, 28733, 7187, 13673, 314, 23452, 12035, 1094, 3537, 342, 25131, 520, 10069, 28733, 7187, 1186, 276, 1822, 20882, 342, 25131, 520, 10069, 28733, 7187, 1186, 9517, 3785, 342, 25131, 520, 10069, 28733, 7187, 1186, 1437, 28707, 293, 26293, 21896, 770, 28716, 342, 25131, 520, 10069, 28733, 7187, 1186, 5185, 324, 342, 25131, 520, 10069, 28733, 7187, 20398, 475, 494, 753, 342, 25131, 520, 10069, 28733, 7187, 20398, 321, 976, 849, 314, 10370, 4902, 314, 342, 25131, 520, 10069, 28733, 7187, 20398, 1294, 401, 353, 12035, 342, 25131, 520, 10069, 28733, 7187, 20398, 3076, 20882, 342, 25131, 520, 10069, 28733, 7187, 16818, 375, 15284, 393, 912, 5895, 16075, 342, 25131, 520, 10069, 28733, 7187, 16818, 375, 1054, 1900, 342, 25131, 520, 10069, 28733, 7187, 8839, 941, 276, 3011, 316, 1822, 342, 25131, 520, 10069, 28733, 7187, 399, 529, 7095, 342, 25131, 520, 10069, 28733, 7187, 399, 941, 491, 342, 25131, 520, 10069, 28733, 7187, 1298, 2166, 13610, 28713, 353, 342, 25131, 520, 10069, 28733, 7187, 318, 770, 28775, 20882, 342, 25131, 520, 10069, 28733, 7187, 318, 770, 28775, 28710, 7187, 28716, 342, 25131, 520, 10069, 28733, 7187, 318, 3439, 20882, 342, 25131, 520, 10069, 28733, 7187, 318, 602, 3011, 342, 25131, 520, 10069, 28733, 7187, 7663, 23764, 265, 28716, 342, 25131, 520, 10069, 28733, 7187, 7663, 28716, 12035, 342, 25131, 520, 10069, 28733, 7187, 7663, 15140, 384, 375, 16105, 342, 25131, 520, 10069, 28733, 7187, 1091, 28716, 689, 912, 524, 8884, 318, 580, 28716, 342, 25131, 520, 10069, 28733, 7187, 1091, 28716, 1186, 276, 270, 342, 25131, 520, 10069, 28733, 7187, 1091, 28724, 28722, 10370, 20325, 342, 25131, 520, 10069, 28733, 7187, 23452, 15284, 342, 25131, 520, 10069, 28733, 7187, 23452, 351, 336, 28712, 342, 25131, 520, 10069, 28733, 7187, 23452, 1298, 2166, 524, 1785, 28710, 4536, 1295, 580, 342, 25131, 520, 10069, 28733, 7187, 23452, 320, 1150, 342, 25131, 520, 10069, 28733, 7187, 23452, 28726, 941, 524, 762, 28710, 342, 25131, 520, 10069, 28733, 7187, 23452, 28726, 19597, 342, 25131, 520, 10069, 28733, 7187, 23452, 313, 384, 529, 420, 265, 28724, 28726, 342, 25131, 520, 10069, 28733, 7187, 1295, 491, 1584, 912, 985, 28729, 1900, 342, 25131, 520, 10069, 28733, 7187, 1295, 1900, 3986, 342, 25131, 520, 10069, 28733, 7187, 985, 2117, 632, 1054, 370, 342, 25131, 520, 10069, 28733, 7187, 1295, 321, 491, 1295, 324, 342, 25131, 520, 10069, 28733, 7187, 25959, 28728, 4499, 4536, 1295, 580, 28708, 342, 25131, 520, 10069, 28733, 7187, 1295, 324, 375, 342, 25131, 520, 10069, 28733, 7187, 1295, 19099, 382, 1150, 3632, 20882, 342, 25131, 520, 10069, 28733, 7187, 1295, 19099, 382, 645, 1035, 28710, 342, 25131, 520, 10069, 28733, 7187, 318, 14475, 384, 1564, 28716, 342, 25131, 520, 10069, 28733, 7187, 18754, 407, 320, 494, 381, 11537, 339, 370, 342, 25131, 520, 10069, 28733, 7187, 18754, 811, 342, 25131, 520, 10069, 28733, 7187, 10677, 6449, 342, 25131, 520, 10069, 28733, 7187, 24901, 420, 431, 28716, 342, 25131, 520, 10069, 28733, 7187, 24901, 8550, 407, 276, 342, 25131, 520, 10069, 28733, 7187, 24901, 318, 602, 1900, 342, 25131, 520, 10069, 28733, 7187, 24901, 318, 602, 28726, 628, 342, 25131, 520, 10069, 28733, 7187, 320, 602, 401, 361, 3533, 28710, 342, 25131, 520, 10069, 28733, 7187, 320, 492, 28716, 524, 316, 1295, 300, 492, 28716, 28733, 7187, 2378, 342, 25131, 520, 10069, 28733, 7187, 320, 492, 28716, 524, 316, 1295, 300, 492, 28716, 28733, 7187, 627, 950, 342, 25131, 520, 10069, 28733, 7187, 14740, 28728, 1230, 24414, 314, 342, 25131, 520, 10069, 28733, 7187, 10347, 1512, 28716, 420, 883, 28716, 1054, 276, 342, 25131, 520, 10069, 28733, 7187, 10347, 1027, 28716, 8549, 16449, 9072, 342, 25131, 520, 10069, 28733, 7187, 320, 536, 28716, 20057, 28710, 342, 25131, 520, 10069, 28733, 7187, 320, 353, 339, 28710, 342, 25131, 520, 10069, 28733, 7187, 550, 1150, 28715, 4499, 342, 25131, 520, 10069, 28733, 7187, 3474, 28710, 13610, 6461, 316, 1186, 1371, 28712, 1294, 28710, 342, 25131, 520, 10069, 28733, 7187, 627, 316, 793, 912, 475, 2015, 1900, 342, 25131, 520, 10069, 28733, 7187, 627, 11056, 5058, 10064, 342, 25131, 520, 10069, 28733, 7187, 1054, 10270, 18595, 342, 25131, 520, 10069, 28733, 7187, 1054, 283, 591, 6449, 342, 25131, 520, 10069, 28733, 7187, 1054, 11513, 399, 504, 6449, 342, 25131, 520, 10069, 28733, 7187, 4891, 1035, 282, 351, 488, 4499, 342, 25131, 520, 10069, 28733, 7187, 1054, 1437, 28707, 370, 491, 342, 25131, 551, 342, 2597, 28716, 28726, 11091, 28733, 7187, 451, 346, 28708, 342, 2597, 28716, 28726, 11091, 28733, 7187, 21918, 2220, 342, 2597, 28716, 5605, 1992, 27357, 382, 23291, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 5605, 20882, 342, 2597, 28716, 3907, 7187, 28716, 28733, 7187, 451, 346, 28708, 342, 2597, 28716, 3907, 7187, 28716, 28733, 7187, 21918, 2220, 342, 2597, 28716, 1294, 20882, 342, 2597, 2176, 15284, 18595, 28733, 28706, 8500, 15010, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 342, 2597, 2176, 20882, 28733, 28706, 14572, 641, 1900, 342, 2597, 2176, 20882, 28733, 28706, 25131, 10064, 342, 2597, 22126, 12517, 342, 2597, 22126, 12517, 342, 2597, 28716, 6260, 342, 2597, 28716, 6260, 342, 351, 1643, 28716, 420, 883, 28716, 342, 6522, 28729, 6522, 28729, 342, 351, 3222, 28716, 524, 602, 370, 342, 351, 3222, 28716, 11729, 849, 1015, 10069, 342, 7670, 283, 4533, 10069, 342, 351, 980, 1724, 7187, 28716, 342, 351, 9366, 9763, 342, 22586, 22025, 342, 22586, 22025, 491, 342, 22586, 28721, 7320, 342, 2597, 1082, 391, 342, 2597, 28764, 1230, 276, 342, 351, 753, 420, 336, 28715, 342, 351, 753, 1186, 883, 28716, 342, 351, 753, 15010, 342, 13609, 5224, 28716, 342, 351, 753, 10069, 342, 351, 753, 10069, 28733, 7187, 11654, 12517, 342, 351, 753, 16950, 342, 11083, 12517, 342, 351, 1230, 342, 5040, 270, 370, 342, 5040, 4533, 10069, 342, 351, 1380, 342, 9154, 10981, 4499, 342, 9154, 524, 265, 28724, 1584, 912, 342, 9154, 23304, 3430, 342, 9154, 338, 491, 491, 342, 9154, 338, 10270, 342, 9154, 278, 912, 342, 9154, 2166, 4827, 342, 9154, 28764, 314, 1371, 6461, 12035, 28733, 7187, 365, 4575, 342, 9154, 28764, 314, 1371, 6461, 12035, 28733, 7187, 20063, 342, 351, 789, 276, 28733, 28706, 451, 346, 28708, 342, 351, 789, 276, 28733, 28706, 21918, 2220, 342, 13609, 8455, 1343, 28716, 342, 351, 11672, 1564, 28716, 28733, 7187, 451, 346, 28708, 342, 351, 11672, 1564, 28716, 28733, 7187, 21918, 2220, 342, 351, 598, 283, 491, 20882, 342, 351, 598, 283, 491, 20882, 342, 351, 598, 283, 491, 20882, 342, 351, 598, 283, 491, 20882, 342, 351, 598, 283, 621, 28716, 342, 351, 598, 283, 621, 28716, 342, 351, 3679, 6748, 20882, 28733, 28706, 420, 2821, 361, 342, 351, 3679, 6748, 20882, 28733, 28706, 475, 375, 21344, 342, 351, 476, 28716, 2654, 375, 28733, 28706, 2378, 342, 13610, 6461, 316, 1186, 293, 17957, 342, 13610, 6461, 316, 4891, 22335, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 342, 13610, 6461, 316, 20882, 28733, 28706, 21918, 2220, 342, 13610, 6461, 316, 28733, 28706, 451, 346, 28708, 342, 6885, 265, 3221, 20882, 342, 13610, 279, 2484, 342, 13610, 28712, 342, 13610, 6448, 20882, 342, 351, 2448, 20882, 342, 351, 2448, 20882, 342, 351, 493, 28716, 2674, 28729, 407, 10069, 342, 351, 493, 407, 283, 20882, 28733, 28706, 8500, 15010, 342, 16387, 2371, 1537, 377, 24264, 342, 16387, 4499, 342, 16387, 339, 28733, 7187, 318, 797, 313, 342, 16387, 28729, 28733, 28706, 15284, 342, 351, 793, 28708, 1010, 267, 28716, 342, 351, 793, 28708, 9573, 329, 342, 351, 793, 28708, 10981, 4499, 342, 351, 793, 1585, 342, 3217, 28768, 28733, 28706, 451, 346, 28708, 342, 3217, 28768, 28733, 28706, 21918, 2220, 342, 6885, 28775, 375, 21344, 342, 6885, 28775, 2654, 375, 28733, 28706, 627, 950, 342, 6885, 28775, 7244, 20882, 342, 4151, 316, 20882, 342, 4151, 316, 20882, 342, 4151, 316, 20882, 28733, 28706, 15018, 912, 318, 14475, 342, 4151, 1152, 28716, 370, 28733, 28706, 451, 346, 28708, 342, 4151, 1152, 28716, 370, 28733, 28706, 21918, 2220, 342, 4151, 591, 491, 342, 4151, 13996, 342, 4151, 591, 28733, 28706, 3268, 28764, 1909, 342, 4151, 591, 28733, 28706, 524, 903, 491, 342, 4151, 27966, 342, 351, 734, 887, 28710, 342, 4151, 28764, 342, 4151, 28764, 753, 342, 351, 6643, 9763, 342, 7292, 407, 12538, 342, 7292, 2933, 20882, 342, 351, 336, 2129, 20882, 342, 351, 336, 28712, 4823, 941, 342, 351, 336, 551, 1374, 28716, 342, 351, 336, 28764, 283, 342, 20915, 2146, 283, 20882, 342, 20915, 2146, 283, 20882, 342, 20915, 2146, 1900, 342, 20915, 2146, 1900, 342, 20915, 2146, 1900, 342, 9713, 353, 342, 7423, 367, 8745, 342, 7423, 10270, 342, 351, 1826, 22657, 342, 7423, 28715, 491, 342, 7423, 28715, 491, 342, 7423, 28715, 28733, 28706, 9167, 4499, 342, 7423, 826, 941, 342, 7423, 5356, 276, 342, 7423, 5356, 276, 342, 7423, 5605, 342, 7423, 28733, 28706, 4823, 941, 28733, 28706, 8500, 15010, 342, 351, 482, 28729, 556, 342, 351, 2821, 912, 28733, 28706, 365, 5544, 3217, 361, 342, 7423, 12517, 342, 7423, 28768, 28733, 28706, 1295, 8775, 491, 342, 7423, 28729, 1029, 342, 7423, 14911, 342, 351, 1426, 9763, 342, 3779, 28775, 276]\n",
      "4381\n"
     ]
    }
   ],
   "source": [
    "r = 668971\n",
    "print(simple[\"train\"][r]['text'])\n",
    "print(tokenizer(simple[\"train\"][r]['text']).input_ids)\n",
    "print(len(tokenizer(simple[\"train\"][r]['text']).input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A list of cities, towns and villages in Fars Province of southern Iran:\n"
     ]
    }
   ],
   "source": [
    "print(simple[\"train\"][r-14]['text']) # I'm tempted to say anything with ten or more |s in it should be thrown out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Suspected infinite loop for row 668971",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m loops \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loops \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m500\u001b[39m:\n\u001b[1;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSuspected infinite loop for row \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(i))\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m#next_tokens = try_add_tokens(rows, next_tokens, debug = i==13814)\u001b[39;00m\n\u001b[0;32m     36\u001b[0m next_tokens \u001b[38;5;241m=\u001b[39m try_add_tokens(rows, next_tokens)\n",
      "\u001b[1;31mValueError\u001b[0m: Suspected infinite loop for row 668971"
     ]
    }
   ],
   "source": [
    "rows = [create_new_row()]\n",
    "\n",
    "max_tokens = 512\n",
    "split_tokens_ref = (842, 28723, 1200, 28725, 342)\n",
    "\n",
    "def try_add_tokens(rows, next_tokens, max_tokens = 512, splitting_tokens = (842, 28723, 1200, 28725, 342)):\n",
    "    current_tokens = rows[-1]['tokens']\n",
    "    if len(current_tokens) + len(next_tokens) <= max_tokens:\n",
    "        rows[-1]['tokens'] += next_tokens\n",
    "        return []\n",
    "    else:\n",
    "        for token in splitting_tokens: # tokens in order of preference\n",
    "            if token in next_tokens:\n",
    "                idx = next_tokens.index(token)\n",
    "                if len(current_tokens) + idx <= max_tokens:\n",
    "                    rows[-1]['tokens'] += next_tokens[:idx+1]\n",
    "                    return next_tokens[idx+1:]\n",
    "    # if we get here, we need to close out the row and start a new one\n",
    "    rows.append(create_new_row())\n",
    "    return next_tokens\n",
    "\n",
    "for i, row in enumerate(simple['train']):\n",
    "    next_tokens = tokenizer(row['text']).input_ids[1:]\n",
    "    loops = 0\n",
    "    while len(next_tokens) > 0:\n",
    "        loops += 1\n",
    "        if loops > 5000:\n",
    "            raise ValueError(\"Suspected infinite loop for row \" + str(i))\n",
    "\n",
    "        #next_tokens = try_add_tokens(rows, next_tokens, debug = i==13814)\n",
    "        next_tokens = try_add_tokens(rows, next_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "for i, row in enumerate(simple['train']):\n",
    "    print(\"Processing input row \" + str(i))\n",
    "    next_tokens = tokenizer(row['text']).input_ids[1:]\n",
    "    loops = 0\n",
    "    while len(next_tokens) > 0:\n",
    "        loops += 1\n",
    "        if loops > 100:\n",
    "            raise ValueError(\"Suspected infinite loop for row \" + str(i))\n",
    "        next_tokens = try_add_tokens(rows, next_tokens)\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "ds = datasets.Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = [create_new_row()]\n",
    "rows[-1][\"tokens\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[1, 2, 3].find(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(df)):\n",
    "    row = df.iloc[i]\n",
    "    if type(row['tokens']) != list:\n",
    "        print(f\"Row {i} is not a list.\")\n",
    "    if len(row['tokens']) > max_tokens:\n",
    "        print(f\"Row {i} is too long at {len(row['tokens'])} tokens.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class BucketSampler(torch.utils.data.Sampler):\n",
    "    \n",
    "    def __init__(self, lengths, buckets=(50,500,50), shuffle=True, batch_size=32, drop_last=False):\n",
    "        \n",
    "        super().__init__(lengths)\n",
    "        \n",
    "        self.shuffle = shuffle\n",
    "        self.batch_size = batch_size\n",
    "        self.drop_last = drop_last\n",
    "        \n",
    "        assert isinstance(buckets, tuple)\n",
    "        bmin, bmax, bstep = buckets\n",
    "        assert (bmax - bmin) % bstep == 0\n",
    "        \n",
    "        buckets = defaultdict(list)\n",
    "        for i, length in enumerate(lengths):\n",
    "            if length > bmin:\n",
    "                bucket_size = min((length // bstep) * bstep, bmax)\n",
    "                buckets[bucket_size].append(i)\n",
    "                \n",
    "        self.buckets = dict()\n",
    "        for bucket_size, bucket in buckets.items():\n",
    "            if len(bucket) > 0:\n",
    "                self.buckets[bucket_size] = torch.tensor(bucket, dtype=torch.int, device='cpu')\n",
    "        \n",
    "        # call __iter__() to store self.length\n",
    "        self.__iter__()\n",
    "            \n",
    "    def __iter__(self):\n",
    "        print(\"__iter__ got called\")\n",
    "        if self.shuffle == True:\n",
    "            for bucket_size in self.buckets.keys():\n",
    "                # shuffle the indices in each bucket\n",
    "                self.buckets[bucket_size] = self.buckets[bucket_size][torch.randperm(self.buckets[bucket_size].nelement())]\n",
    "                \n",
    "        batches = []\n",
    "        for bucket in self.buckets.values(): # iterate over the buckets\n",
    "            curr_bucket = torch.split(bucket, self.batch_size) # split the bucket into batches\n",
    "            if len(curr_bucket) > 1 and self.drop_last == True: # handle drop_last\n",
    "                if len(curr_bucket[-1]) < len(curr_bucket[-2]):\n",
    "                    curr_bucket = curr_bucket[:-1]\n",
    "            batches += curr_bucket # add the batches to the list of all batches\n",
    "            \n",
    "        self.length = len(batches) # the length of a Sampler might be used to let the DataLoader know how many batches it should expect\n",
    "        \n",
    "        if self.shuffle == True:\n",
    "            random.shuffle(batches) # shuffle the batches\n",
    "            \n",
    "        return iter(batches) # return an iterator over the batches\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "# add a length column to the dataset\n",
    "simple['train'] = simple['train'].map(lambda x: {'lengths': len(x['text'].split())})\n",
    "simple['train']\n",
    "bucket_dataloader = torch.utils.data.DataLoader(simple['train'], batch_sampler=BucketSampler(simple['train']['lengths'], batch_size=32, shuffle=True, drop_last=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "buckets = BucketSampler(simple['train']['lengths']).buckets\n",
    "for k, v in buckets.items():\n",
    "    print(k, v.shape)\n",
    "\n",
    "# So I'm actually a bit confused.  Oh I see, the code Copilot wrote for me splits on spaces.  So this is kind of an approximate tokenization.\n",
    "# So the way this works is that the sampler (1) receives the entire datas set and (2) I guess handles all the shuffling and batching itself?\n",
    "\n",
    "# Okay, so how did that rabbit hole all come up?  It was because I was trying to understand exactly where the tokenization can fit into the process, and how we trade off efficiency and flexibility.\n",
    "# Oh...what happens if we just wrap the Hugging Face dataset?  Okay it spits out a dictionary.\n",
    "\n",
    "# So what's next?  I think you essentially always want to using a torch-tokenized dataset as the input for the model, because we have demonstrated that even converting lists of ints to tensors takes non-negligible time.\n",
    "# In addition, you can't do any bucketing or anything like that if you don't have the tokenized dataset.\n",
    "# Now there's a downside, which is that in order to tokenize the dataset with torch, you pretty much have to pad it.\n",
    "\n",
    "# Anyways, regardless of that we have some preprocessing to do.  The simple way is to just bucket everything and pretend document breaks don't matter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "gpt2t = AutoTokenizer.from_pretrained('gpt2')\n",
    "mistralt = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tallies = {\n",
    "    'gpt2_tokens': 0,\n",
    "    'mistral_tokens': 0,\n",
    "    'chars': 0\n",
    "}\n",
    "for row in simple['train']:\n",
    "    text = row['text']\n",
    "    tallies['gpt2_tokens'] += len(gpt2t.tokenize(text))\n",
    "    tallies['mistral_tokens'] += len(mistralt.tokenize(text))\n",
    "    tallies['chars'] += len(text)\n",
    "\n",
    "print(tallies) # it's actually around 2.7!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "37.5/14/1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "hf_loader = DataLoader(simple['train'], batch_size=32, shuffle=True, drop_last=False)\n",
    "\n",
    "for batch in hf_loader:\n",
    "    print(batch)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def dynamic_length_collate_fn(lst):\n",
    "    text = [x[\"text\"] for x in lst]\n",
    "    max_len = max(len(x) for x in lst)\n",
    "    return tokenizer(\n",
    "        text,\n",
    "        padding=\"longest\",\n",
    "        return_tensors=\"pt\",\n",
    "    ).input_ids\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "dataloader = DataLoader(simple[\"train\"], batch_size = 32, collate_fn = dynamic_length_collate_fn)\n",
    "for i, batch in enumerate(dataloader):\n",
    "    print(batch.shape)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay, so apparently there is a default \"RandomSampler\"\n",
    "# Note: RandomSampler(train_ds,replacement=True).  You pass both the Dataset itself and a sampler that points to the Dataset to the loader.\n",
    "class weightedsampler(Sampler):\n",
    "    \n",
    "    def __init__(self,dataset):\n",
    "        \n",
    "        self.indices = list(range(len(dataset)))\n",
    "        self.num_samples = len(dataset)\n",
    "        self.label_to_count = dict(Counter(dataset.bins))\n",
    "        weights = [1/self.label_to_count[i] for i in dataset.bins]\n",
    "        \n",
    "        self.weights = torch.tensor(weights,dtype=torch.double)\n",
    "        \n",
    "    def __iter__(self):\n",
    "        count = 0\n",
    "        index = [self.indices[i] for i in torch.multinomial(self.weights, self.num_samples, replacement=True)]\n",
    "        while count < self.num_samples:\n",
    "            yield index[count]\n",
    "            count += 1\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "        \n",
    "\n",
    "# So it looks like when the sampler is initialized on the Dataset, it analyzes the Dataset, inspects a property called dataset.bins (which I think is custom?) in order to save a tensor of weights.\n",
    "# the actual weighting logic is used from torch.multinomial which seems to be built for this sort of thing.  Then it defines __iter__ (with yield) and __len__ which I assume are needed for it to be a Sampler.\n",
    "# Okay, and it sounds like Samplers are used only with map-style datasets.\n",
    "# Alright, if you set both batch_size and sampler to None, then the collate_fn sees an individual data sample, which doesn't sound useful.\n",
    "# Okay it sounds like maybe I should always set pin_memory to True?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://pytorch.org/docs/stable/data.html#data-loading-order-and-sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_estimate_tokens(ds, field=\"text\", already_tokenized = False, chars_per_token=4):\n",
    "    tally = 0\n",
    "    max_len = 0\n",
    "    lengths = {}\n",
    "    for row in ds:\n",
    "        l = len(row[field])\n",
    "        tally += l\n",
    "        lengths[l] = lengths.get(l, 0) + 1\n",
    "        if l > max_len:\n",
    "            max_len = l\n",
    "\n",
    "    print(f'{int(tally):_}')\n",
    "    print(f'Max length: {max_len}, estimated tokens: {int(max_len / chars_per_token):_}')\n",
    "    lengths = list(lengths.items())\n",
    "    lengths.sort(reverse=True)\n",
    "    return int(tally), lengths\n",
    "\n",
    "total, length = quick_estimate_tokens(simple['train'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(length, columns=['Length', 'Count'])\n",
    "#histogram\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "def map_tokenize_as_list(x):\n",
    "    return tokenizer(x['text'])\n",
    "\n",
    "\n",
    "\n",
    "list_tokenized = simple['train'].map(map_tokenize_as_list, batched=True, remove_columns=['text']) # That takes 30 seconds to tokenize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def map_tokenize_as_tensors(x):\n",
    "    return tokenizer(x['text'], return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tensor_tokenized = simple['train'].map(map_tokenize_as_tensors, batched=True, remove_columns=['text']) # That takes 2 whole minues to tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "for batch in tokenizing_dataloader:\n",
    "    pass\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_estimate_tokens(tokenized['train'], field=\"input_ids\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_profile(ds, field, print_every = None)\n",
    "    \"\"\" This should operate on an already-tokenized dataset. \"\"\"\n",
    "    freqs = {}\n",
    "    # can we do this with datasets.map?\n",
    "    n = len(ds)\n",
    "    print(f\"Processing {n} examples\")\n",
    "    for idx, ex in enumerate(ds):\n",
    "        if print_every is not None and idx % print_every == 0:\n",
    "            print(f\"Processed {idx} examples\")\n",
    "\n",
    "        if \n",
    "        for t in tokens:\n",
    "            print(t)\n",
    "            if t in freqs:\n",
    "                freqs[t] += 1\n",
    "            else:\n",
    "                freqs[t] = 1\n",
    "\n",
    "        break\n",
    "\n",
    "    total = sum(freqs.values())\n",
    "    desc = [(tknzr.decode(k), v/total) for k, v in freqs.items()]\n",
    "    return total, desc, freqs\n",
    "\n",
    "\n",
    "import datasets\n",
    "simple = \"rahular/simple-wikipedia\"\n",
    "simple = datasets.load_dataset(simple)\n",
    "simple\n",
    "\n",
    "from data import WrappedHuggingFaceTokenizer\n",
    "tokenizer = WrappedHuggingFaceTokenizer(WrappedHuggingFaceTokenizer.MISTRAL)\n",
    "\n",
    "total, ordered, freqs = token_profile(simple['train'], 'text', tokenizer, print_every = 100_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay.  So, question.  Attention mask.  Ah okay, so Hugging Face expects the data collator to take care of the masking.\n",
    "# And I guess the reason so many tokenizers don't have default pad_tokens is that they were trained without them and Hugging Face wants to serve up exactly what was used in training.\n",
    "# So now I understand \"why the fence is there\"; now how do I change it?\n",
    "# So there's a bit of a dilemma here because I want to be able to use Hugging Face's tokenizers, but I don't want to depend on their overall data collation strategy.\n",
    "# I could wrap their tokenizer in my own class that changes eos_token to -100.  Ummm...will the embedding choke on that?  I believe it will.  Well, a pretrained embedding will, anyway.\n",
    "# There's going to be a potentially serious issue here where comparisons to pretrained models are really hard.  But maybe I can set up a wrapper for that.\n",
    "# So, question about padding strategies.  I'm currently using the nn.Transformers method that creates causal attention masks.\n",
    "# One thing I could just do is assume -100 is always the padding token.  If I do that, then I also have to...well, hold on.  If we left-pad, then things get weird and I have to use an attention mask.\n",
    "# But I think if I right-pad, everything is fine?  You'll waste a bit of time calculating attention for the padded tokens in batches where not eveyrthing is the same length, but I think you have to do that anyway.\n",
    "# I think left-padding is a strategy for .generate()?  I think that's it.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
