{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 1,  0, -1],\n",
      "        [-2, -3, -4]])\n",
      "tensor([[0.5000],\n",
      "        [1.5000]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# make a tensor out of a range\n",
    "a = torch.tensor(range(6)).reshape(2, 3)\n",
    "b = torch.tensor(range(1, -5, -1)).reshape(2, 3)\n",
    "c = torch.tensor([0.5, 1.5]).unsqueeze(1)\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "identity\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "transpose\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "tensor([[0, 3],\n",
      "        [1, 4],\n",
      "        [2, 5]])\n",
      "row sum\n",
      "tensor([ 3, 12])\n",
      "tensor([ 3, 12])\n",
      "column sum\n",
      "tensor([3, 5, 7])\n",
      "tensor([3, 5, 7])\n",
      "sum\n",
      "tensor(15)\n",
      "tensor(15)\n"
     ]
    }
   ],
   "source": [
    "# unary operations\n",
    "print(\"identity\")\n",
    "print(a)\n",
    "print(torch.einsum('ij->ij', a))\n",
    "print(torch.einsum('ij', a))\n",
    "print(\"transpose\")\n",
    "print(a.T)\n",
    "print(torch.einsum('ij->ji', a))\n",
    "print(\"row sum\")\n",
    "print(a.sum(dim=1))\n",
    "print(torch.einsum('ij->i', a))\n",
    "print(\"column sum\")\n",
    "print(a.sum(dim=0))\n",
    "print(torch.einsum('ij->j', a))\n",
    "print(\"sum\")\n",
    "print(a.sum())\n",
    "print(torch.einsum('ij->', a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "einsum(): subscript i is repeated for operand 0 but the sizes don't match, 3 != 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# you can also do the following, apparently\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mij\u001b[39m\u001b[38;5;124m'\u001b[39m, a))\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39meinsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mii\u001b[39m\u001b[38;5;124m'\u001b[39m, a))\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/torch/functional.py:380\u001b[0m, in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m einsum(equation, \u001b[38;5;241m*\u001b[39m_operands)\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(operands) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39menabled:\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;66;03m# the path for contracting 0 or 1 time(s) is already optimized\u001b[39;00m\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;66;03m# or the user has disabled using opt_einsum\u001b[39;00m\n\u001b[0;32m--> 380\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _VF\u001b[38;5;241m.\u001b[39meinsum(equation, operands)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    382\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_einsum\u001b[38;5;241m.\u001b[39mis_available():\n",
      "\u001b[0;31mRuntimeError\u001b[0m: einsum(): subscript i is repeated for operand 0 but the sizes don't match, 3 != 2"
     ]
    }
   ],
   "source": [
    "# you can also do the following, apparently\n",
    "\n",
    "print(torch.einsum('ii', a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 1,  0, -1],\n",
      "        [-2, -3, -4]])\n",
      "hadamard product\n",
      "tensor([[  0,   0,  -2],\n",
      "        [ -6, -12, -20]])\n",
      "tensor([[  0,   0,  -2],\n",
      "        [ -6, -12, -20]]) tensor([[  0,   0,  -2],\n",
      "        [ -6, -12, -20]])\n",
      "hadamard product row sum\n",
      "tensor([ -2, -38])\n",
      "tensor([ -2, -38])\n",
      "hadamard product column sum\n",
      "tensor([ -6, -12, -22])\n",
      "tensor([ -6, -12, -22])\n",
      "hadamard product sum\n",
      "tensor(-40)\n",
      "tensor(-40)\n",
      "hadamard product transpose\n",
      "tensor([[  0,  -6],\n",
      "        [  0, -12],\n",
      "        [ -2, -20]])\n",
      "tensor([[  0,  -6],\n",
      "        [  0, -12],\n",
      "        [ -2, -20]])\n"
     ]
    }
   ],
   "source": [
    "# binary operations: Hadamard products\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"hadamard product\")\n",
    "print(a*b)\n",
    "print(torch.einsum('ij,ij->ij', a, b), a*b)\n",
    "print(\"hadamard product transpose\")\n",
    "print((a*b).T)\n",
    "print(torch.einsum('ij,ij->ji', a, b)) \n",
    "print(\"hadamard product row sum\")\n",
    "print((a*b).sum(dim=1))\n",
    "print(torch.einsum('ij,ij->i', a, b))\n",
    "print(\"hadamard product column sum\")\n",
    "print((a*b).sum(dim=0))\n",
    "print(torch.einsum('ij,ij->j', a, b))\n",
    "print(\"hadamard product sum\")\n",
    "print((a*b).sum())\n",
    "print(torch.einsum('ij,ij->', a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 1,  0, -1],\n",
      "        [-2, -3, -4]])\n",
      "a x b^T\n",
      "tensor([[ -2, -11],\n",
      "        [ -2, -38]])\n",
      "tensor([[ -2, -11],\n",
      "        [ -2, -38]])\n",
      "a^T x b\n",
      "tensor([[ -6,  -9, -12],\n",
      "        [ -7, -12, -17],\n",
      "        [ -8, -15, -22]])\n",
      "tensor([[ -6,  -9, -12],\n",
      "        [ -7, -12, -17],\n",
      "        [ -8, -15, -22]])\n",
      "a x b.T row sum\n",
      "tensor([-13, -40])\n",
      "tensor([-13, -40])\n",
      "a^T x b column sum\n",
      "tensor([-21, -36, -51])\n",
      "tensor([-21, -36, -51])\n",
      "a x b.T sum\n",
      "tensor(-53)\n",
      "tensor(-53)\n",
      "a^T x b row sum\n",
      "tensor(-108)\n",
      "tensor(-108)\n",
      "a x b.T column sum\n",
      "tensor(-53)\n",
      "tensor(-53)\n",
      "a^T x b sum\n",
      "tensor(-108)\n",
      "tensor(-108)\n"
     ]
    }
   ],
   "source": [
    "# binary operations: transposed matrix multiplication\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"a x b^T\")\n",
    "print(a @ b.T)\n",
    "print(torch.einsum('ij,kj->ik', a, b))\n",
    "print(\"a^T x b\")\n",
    "print(a.T @ b)\n",
    "print(torch.einsum('ij,ik->jk', a, b))\n",
    "print(\"a x b.T row sum\")\n",
    "print((a @ b.T).sum(dim=1))\n",
    "print(torch.einsum('ij,kj->i', a, b))\n",
    "print(\"a^T x b column sum\")\n",
    "print((a.T @ b).sum(dim=0))\n",
    "print(torch.einsum('ij,ik->k', a, b))\n",
    "print(\"a x b.T sum\")\n",
    "print((a @ b.T).sum())\n",
    "print(torch.einsum('ij,kj->', a, b))\n",
    "print(\"a^T x b row sum\")\n",
    "print((a.T @ b).sum())\n",
    "print(torch.einsum('ij,ik->', a, b))\n",
    "print(\"a x b.T column sum\")\n",
    "print((a @ b.T).sum())\n",
    "print(torch.einsum('ij,kj->', a, b))\n",
    "print(\"a^T x b sum\")\n",
    "print((a.T @ b).sum())\n",
    "print(torch.einsum('ij,ik->', a, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 1,  0, -1],\n",
      "        [-2, -3, -4]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[  0,   0,   0],\n",
       "         [  1,   0,  -1],\n",
       "         [  2,   0,  -2]],\n",
       "\n",
       "        [[ -6,  -9, -12],\n",
       "         [ -8, -12, -16],\n",
       "         [-10, -15, -20]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I am unclear on this one\n",
    "print(a)\n",
    "print(b)\n",
    "torch.einsum('ij,ik->ijk', a, b) # the way Bing describes this is the element at i,j,k is a[i,j] * b[i,k].  Okay so actually..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 1, 2],\n",
      "        [3, 4, 5]])\n",
      "tensor([[ 1,  0, -1],\n",
      "        [-2, -3, -4]])\n",
      "Kronecker product, sort of\n",
      "tensor([[  0,   0,   0,   1,   0,  -1,   2,   0,  -2],\n",
      "        [  0,   0,   0,  -2,  -3,  -4,  -4,  -6,  -8],\n",
      "        [  3,   0,  -3,   4,   0,  -4,   5,   0,  -5],\n",
      "        [ -6,  -9, -12,  -8, -12, -16, -10, -15, -20]]) torch.Size([4, 9])\n",
      "tensor([[[[  0,   0,   0],\n",
      "          [  0,   0,   0]],\n",
      "\n",
      "         [[  1,   0,  -1],\n",
      "          [ -2,  -3,  -4]],\n",
      "\n",
      "         [[  2,   0,  -2],\n",
      "          [ -4,  -6,  -8]]],\n",
      "\n",
      "\n",
      "        [[[  3,   0,  -3],\n",
      "          [ -6,  -9, -12]],\n",
      "\n",
      "         [[  4,   0,  -4],\n",
      "          [ -8, -12, -16]],\n",
      "\n",
      "         [[  5,   0,  -5],\n",
      "          [-10, -15, -20]]]]) torch.Size([2, 3, 2, 3])\n",
      "tensor([[  0,   0,   0,   0,   0,   0,   1,   0,  -1],\n",
      "        [ -2,  -3,  -4,   2,   0,  -2,  -4,  -6,  -8],\n",
      "        [  3,   0,  -3,  -6,  -9, -12,   4,   0,  -4],\n",
      "        [ -8, -12, -16,   5,   0,  -5, -10, -15, -20]])\n"
     ]
    }
   ],
   "source": [
    "# binary operations: outer product\n",
    "print(a)\n",
    "print(b)\n",
    "print(\"Kronecker product, sort of\")\n",
    "print(torch.kron(a, b), torch.kron(a, b).shape)\n",
    "print(torch.einsum('ij,kl->ijkl', a, b), torch.einsum('ij,kl->ijkl', a, b).shape)\n",
    "# So it occurs to me that in addition to einsum, my skills with reshape are also weak\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ij,ij->ij tensor(True)\n",
      "ij,ij->ji tensor(True)\n",
      "ij,ij->i tensor(True)\n",
      "ij,ij->j tensor(True)\n",
      "ij,ij-> tensor(True)\n",
      "ij,ik->ijk tensor(True)\n",
      "ij,kl->ijkl tensor(True)\n"
     ]
    }
   ],
   "source": [
    "# A manual loop that performs einsum, minus several options such as diagonals.\n",
    "import itertools\n",
    "def loop_einsum(s, a, b):\n",
    "    in_string, out_string = s.split('->')\n",
    "    in_string1, in_string2 = in_string.split(',')\n",
    "    in_string1, in_string2, out_string = list(in_string1), list(in_string2), list(out_string)\n",
    "    inter_string = in_string1 + [letter for letter in in_string2 if letter not in in_string1]\n",
    "    inter_dims = list(a.shape)\n",
    "    for i, letter in enumerate(in_string2):\n",
    "        if letter not in in_string1:\n",
    "            inter_dims.append(b.shape[i])\n",
    "    inter_vals = torch.zeros(inter_dims)\n",
    "    coords = list(itertools.product(*[range(dim) for dim in inter_dims]))\n",
    "    # element-wise multiplication\n",
    "    for coord in coords:\n",
    "        coords_a = [coord[i] for i, letter in enumerate(inter_string) if letter in in_string1]\n",
    "        coords_b = [coord[i] for i, letter in enumerate(inter_string) if letter in in_string2]\n",
    "        inter_vals[coord] = a[*coords_a] * b[*coords_b]\n",
    "    # summation\n",
    "    sum_dims = tuple([i for i, letter in enumerate(inter_string) if letter not in out_string])\n",
    "    if len(sum_dims) == 0:\n",
    "        out_vals = inter_vals\n",
    "    else:\n",
    "        out_vals = inter_vals.sum(dim=sum_dims)\n",
    "    # transpose\n",
    "    ordered_out_string = [letter for letter in inter_string if letter in out_string]\n",
    "    if tuple(ordered_out_string) != tuple(out_string):\n",
    "        out_vals = out_vals.permute(*[ordered_out_string.index(letter) for letter in out_string])\n",
    "    return out_vals\n",
    "\n",
    "import torch\n",
    "a = torch.tensor(range(6)).reshape(2, 3)\n",
    "b = torch.tensor(range(1, -5, -1)).reshape(2, 3)\n",
    "test_cases = [\n",
    "    \"ij,ij->ij\",\n",
    "    \"ij,ij->ji\",\n",
    "    \"ij,ij->i\",\n",
    "    \"ij,ij->j\",\n",
    "    \"ij,ij->\",\n",
    "    \"ij,ik->ijk\",\n",
    "    \"ij,kl->ijkl\",\n",
    "]\n",
    "for test_case in test_cases:\n",
    "    canonical = torch.einsum(test_case, a, b)\n",
    "    mine = loop_einsum(test_case, a, b)\n",
    "    print(test_case, (canonical==mine).all())\n",
    "\n",
    "# All my tests pass, and I think I understand it now.  So the one thing I didn't implement were diagonals...how does that work again?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alright so what does this mean in terms of attention and linear attention?\n",
    "#KV = torch.einsum(\"nshd,nshm->nhmd\", K, values)\n",
    "# exactly one unshared dimension means it's a matmul, I think.  Quadratic, but scaling with model dimension rather than sequence length.\n",
    "\n",
    "#Z = 1/(torch.einsum(\"nlhd,nhd->nlh\", Q, K.sum(dim=1))+self.eps)\n",
    "# dropping one dimension, with all the others shared, I think means a Hadamard product followed by a sum, so that's in linear time.\n",
    "# V = torch.einsum(\"nlhd,nhmd,nlh->nlhm\", Q, KV, Z) \n",
    "# uh oh, triple threat.  This one might be another matmul scaling with model dim."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
